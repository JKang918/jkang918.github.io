---
title: Time Series > MA, AR, ARMA, ARIMA
date: 2025-08-10 #HH:MM:SS +/-TTTT
categories: [Econometrics, Time Series]
tags: [econometrics, time series, arima]     # TAG names should always be lowercase
author: <author_id>                     # for single entry
description: Stationarity, Ergodicity, Impulse Response Analysis
toc: true
math: true
mermaid: true
comments: true
---

**Note**
- "Stationary" means "Weak Stationary" in this post unless otherwise stated.
- Weak Staiontionary $\iff$ Ergodic for 2nd momenets.
- If $\sum_{j=0}^{\infty} \lvert \gamma_j \rvert < \infty$, then $y_t$ is ergodic for 1st moments.
- Error term(s) follow **white noise** unless otherwise stated, i.e.,

$$
E[\varepsilon_t] = 0
$$

$$
\mathrm{Var}(\varepsilon_t) = \sigma^2
$$

$$
\mathrm{Cov}(\varepsilon_t, \varepsilon_\tau) = 0, \text{ where } t \neq \tau
$$



## Moving Average (MA)


$$
y_t = \mu + \varepsilon_t + \beta_1 \varepsilon_{t-1} + \beta_2 \varepsilon_{t-2} + \; \cdots \; + \beta_q \varepsilon_{t-q}
$$

### MA(1)

$$
y_t = \mu + \varepsilon_t + \beta_1 \varepsilon_{t-1}
$$

**MA(1) is stationary process:**

**(1) Mean**:
$$
E[y_t] = E[\mu + \varepsilon_t + \beta_1 \varepsilon_{t-1}] = \mu
$$

$\mu$ is a constant term, which satisfies $< \infty$ (converge)


**(2) Variance**:


$$
\begin{aligned}
\mathrm{Var}(y_t) &= E[(y_t - \mu)^2] \\
&= E[(\varepsilon_t + \beta_1 \varepsilon_{t-1})^2] \\
&= E[\varepsilon_t^2] + \beta_1^2 E[\varepsilon_{t-1}^2] \\
&= \sigma^2(1 + \beta_1^2)
\end{aligned}
$$

$\sigma^2(1 + \beta_1^2)$ is a constant term, which satisfies $< \infty$ (converge)


**(3) Autocovariance** ($j \neq 0$):


$$
\begin{aligned}
\gamma_j &= E[(y_t - \mu)(y_{t-j} - \mu)] \\
&= E[(\varepsilon_t + \beta_1 \varepsilon_{t-1})(\varepsilon_{t-j} + \beta_1 \varepsilon_{t-j-1})] \\
&= \beta_j \sigma^2
\end{aligned}
$$

where

$$
\beta_j =
\begin{cases}
\beta_1, & j = 1 \\
0, & j > 1
\end{cases}
$$


Either way, $\beta_j \sigma^2$ satisfies $< \infty$ (converge)

Thus, MA(1) is stationary process. ($\iff$ ergodic for 2nd moments )


**MA(1) is ergodic for 1st moments:**


$$
\sum_{j=0}^\infty |\gamma_j| = |\gamma_0| + |\gamma_1| + |\gamma_2| + \dots \\
= |\gamma_0| + |\gamma_1| =\sigma^2(1 + \beta_1^2) + |\beta_1| \sigma^2 < \infty
$$

Hence MA(1) is ergodic for the 1st moments.

**AutoCorrelation Function for MA(1):**

- For $j = 1$:
$$
\rho_1 = \frac{\gamma_1}{\gamma_0} = \frac{\beta_1 \sigma^2}{(1 + \beta_1^2)\sigma^2} = \frac{\beta_1}{1 + \beta_1^2}
$$

- For $j > 1$:
$$
\rho_j = 0
$$

---

### MA(q)

$$
y_t = \mu + \varepsilon_t + \beta_1 \varepsilon_{t-1} + \dots + \beta_q \varepsilon_{t-q}
$$


**MA(q) is stationary process:**


**(1) Mean**:

$$
E[y_t] = \mu
$$

$\mu$ is a constant term, which satisfies $< \infty$ (converge)


**(2) Variance**:

$$
\mathrm{Var}(y_t) = \left(1 + \beta_1^2 + \dots + \beta_q^2 \right)\sigma^2
$$

$\left(1 + \beta_1^2 + \dots + \beta_q^2 \right)\sigma^2$ is a constant term, which satisfies $< \infty$ (converge)


**(3) Autocovariance** ($j \neq 0$):

$$
\gamma_j =
\begin{cases}
(\dots \rightarrow \text{ square terms of } \beta_i)\sigma^2, & j \leq q \\
0, & j > q
\end{cases}
$$

Thus, MA(q) is stationary process. ($\iff$ ergodic for 2nd moments )

**MA(q) is ergodic for 1st moments:**


$$
\sum_{j=0}^\infty |\gamma_j| = |\gamma_0| + |\gamma_1| + |\gamma_2| + \dots \\
 < \infty \; \text{(per above)}
$$

Hence MA(q) is ergodic for the 1st moments.

**AutoCorrelation Function for MA(q):**

- For $j \leq q$:
$$
\rho_j = \gamma_j / \gamma_0 \qquad \text{(specification can be deduced from above)} 
$$

- For $j > q$:
$$
\rho_j = 0
$$

---

### MA(∞)

$$
y_t = \mu + \beta_0 \varepsilon_t + \beta_1 \varepsilon_{t-1} + \dots + \beta_q \varepsilon_{t-q} + \cdots
$$


**MA(∞) is stationary process with a certain condition:**


**(1) Mean**:

$$
E[y_t] = \mu
$$

$\mu$ is a constant term, which satisfies $< \infty$ (converge)


**(2) Variance**:

$$
\begin{align*}
    \mathrm{Var}(y_t) &= E[(y_t - \mu)^2] \\
                    &= E[(\varepsilon_t + \beta_1 \varepsilon_{t-1} + \cdots)^2] \\
                    &= E[\varepsilon_t^2 + \beta_1^2 \varepsilon_{t-1}^2 + \cdots] \\
                    &= (1 + \beta_1^2 + \beta_2^2 + \cdots)E[\varepsilon_t^2]\\
                    &= \sigma^2(1 + \beta_1^2 + \beta_2^2 + \cdots) \\
                    &= \lim_{T\to\infty} \sigma^2(1 + \beta_1^2 + \beta_2^2 + \cdots + \beta_T^2) \\
\end{align*}
$$

$\mathrm{Var}(y_t)$ converges for $\sum_{j=0}^{\infty} \lvert \beta_j \rvert < \infty$.

$\lvert \beta_j \rvert < 1, \; \forall j$ is a sufficient condition for $\sum_{j=0}^{\infty} \lvert \beta_j \rvert < \infty$.


**(3) Autocovariance** ($j \neq 0$):

$$
\gamma_j = \sigma^2(\beta_j + \beta_{j+1}\beta_1 + \dots)
$$

$\sigma^2(\beta_j + \beta_{j+1}\beta_1 + \dots)$ converges for $\sum_{j=0}^{\infty} \lvert \beta_j \rvert < \infty$.

Thus, MA(∞) is stationary process as long as $\sum_{j=0}^{\infty} \lvert \beta_j \rvert < \infty$.  ($\iff$ ergodic for 2nd moments)

**MA(∞) is ergodic for 1st moments with a certain condition:**


$$
\sum_{j=0}^\infty |\gamma_j| = |\mathrm{Var}(y_t)| + \sum_{j=1}^\infty |\gamma_j| \\
 < \infty \; \text{(per above)}
$$

Hence MA(q) is ergodic for the 1st moments as long as $\sum_{j=0}^{\infty} \lvert \beta_j \rvert < \infty$.

**AutoCorrelation Function for MA(∞):**

$$
\rho_j = \gamma_j / \gamma_0 \qquad \text{(specification can be deduced from above)} 
$$


---
---

## AutoRegressive (AR)


$$
y_t = \alpha_0 + \alpha_1 y_{t-1} + \alpha_2 y_{t-2} + \; \cdots \; + \alpha_q y_{t-p} + \varepsilon_t
$$

### AR(1)

$$
y_t = \alpha_0 + \alpha_1 y_{t-1} + \varepsilon_t
$$

**AR(1) is MA(∞) ($\lvert \alpha_1 \rvert < 1$):**

$$
\begin{align*}
y_t &= \alpha_0 + \alpha_1 y_{t-1} + \varepsilon_t \\
&= \alpha_0 + \alpha_1 \left( \alpha_0 + \alpha_1 y_{t-2} + \varepsilon_{t-1} \right) + \varepsilon_t \\
&= \alpha_0 (1 + \alpha_1) + \alpha_1^2 y_{t-2} + \alpha_1 \varepsilon_{t-1} + \varepsilon_t \\
&= \alpha_0 (1 + \alpha_1 + \alpha_1^2) + \alpha_1^3 y_{t-3} + \alpha_1^2 \varepsilon_{t-2} + \alpha_1 \varepsilon_{t-1} + \varepsilon_t \\
&\quad \vdots \\
&= \alpha_0 \sum_{j=0}^\infty \alpha_1^j + \alpha_1^\infty y_{t-\infty} + \sum_{j=0}^\infty \alpha_1^j \varepsilon_{t-j} \\
&= \frac{\alpha_0}{1-\alpha_1} + \sum_{j=0}^\infty \alpha_1^j \varepsilon_{t-j} \qquad (\lvert \alpha_1 \rvert < 1 \Rightarrow \lvert \beta_j \rvert < 1 \Rightarrow \sum_{j=0}^{\infty} \lvert \beta_j \rvert < \infty)
\end{align*}
$$

Therefore, as we concluded in the previous section that MA(∞) is stationary and ergodic under a certain condition, AR(1) is also stationary and ergodic when $\lvert \alpha_1 \rvert < 1$.


> This is actually an example of "Wald Representation Theorem", which is a given stationary process can be expressed as MA(∞).

**AR(1) is stationary process ($\lvert \alpha_1 \rvert < 1$):**


**(1) Mean**:

$$
E[y_t] = \frac{\alpha_0}{1-\alpha_1}
$$

$\frac{\alpha_0}{1-\alpha_1}$ is a constant term, which satisfies $< \infty$ (converge).

$\frac{\alpha_0}{1-\alpha_1}$ is equivalent of $\mu$ in MA(∞).

**(2) Variance**:

$$
\mathrm{Var}(y_t) = \frac{\sigma^2}{1-\alpha_1^2}
$$

$\frac{\sigma^2}{1-\alpha_1^2}$ is a constant term, which satisfies $< \infty$ (converge)


**(3) Autocovariance** ($j \neq 0$):

$$
\gamma_j = \frac{\alpha_1^j}{1-\alpha_1^2}\sigma^2
$$

$\frac{\alpha_1^j}{1-\alpha_1^2}\sigma^2$ is a constant term, which satisfies $< \infty$ (converge)

Thus, AR(1) is stationary process. ($\iff$ ergodic for 2nd moments )

**AR(1) is ergodic for 1st moments:**


$$
\sum_{j=0}^\infty |\gamma_j| = |\gamma_0| + |\gamma_1| + |\gamma_2| + \dots \\
 < \infty \; \text{(per above)}
$$

Hence AR(1) is ergodic for the 1st moments.

**AutoCorrelation Function for AR(1):**

- For $j \leq q$:
$$
\rho_j = \alpha_1^j
$$

> Remark: as $j$ goes up, covariance and correlation drops in AR(1)


### AR(2)

We go over AR(2) before AR(p) to familiarize with the usage of lag operator.

Without the loss of generality, we can disregard for now the constant term.

$$
y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \varepsilon_t
$$

**Solution for $y_t$ using lag operator**

$$
\begin{align*}
  &y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \varepsilon_t \\
  &y_t - \phi_1 y_{t-1} - \phi_2 y_{t-2} = \varepsilon_t \\
  &y_t - \phi_1 L y_{t} - \phi_2 L^2 y_{t} = \varepsilon_t \\   
  &(1 - \phi_1 L - \phi_2 L^2) y_{t} = \varepsilon_t \\
  & y_{t} = (1 - \phi_1 L - \phi_2 L^2)^{-1} \varepsilon_t \\
  & (\therefore) \; y_{t} = [(1 - \lambda_1 L) (1 - \lambda_2 L^2)]^{-1} \varepsilon_t \\    
\end{align*}
$$

where $\lambda_1$ and $\lambda_2$ are solution for $P(L) = 1 - \phi_1 L - \phi_2 L^2 = 0$ and $\| \mathbf{\lambda} \| < 1$ s.t. $\mathbf{\lambda} = (\lambda_1, \; \lambda_2)$ 

Then, AR(2) with the constant term can be expressed as follows:

$$
\begin{align*}
  y_t &= \alpha_0 + \alpha_1 y_{t-1} + \alpha_2 y_{t-2} + \varepsilon_t \\ 
      &= \frac{\alpha_0}{1 - \alpha_1 - \alpha_2} + \frac{\varepsilon_t}{1 - \alpha_1 L - \alpha_2 L^2}
\end{align*}
$$


**AR(2) is stationary process ($\lvert \mathbf{\lambda} \rvert < 1$):**

$\lambda_1$ and $\lambda_2$ are solutions for $1-\alpha_1 L - \alpha_2 L^2 = 0$.

**(1) Mean**:

$$
E[y_t] = \frac{\alpha_0}{1 - \alpha_1 - \alpha_2} = \mu
$$

$\frac{\alpha_0}{1 - \alpha_1 - \alpha_2}$ is a constant term, which satisfies $< \infty$ (converge).


**(2) Autocovariance** ($j \neq 0$):

For AR(2), let's check covariance first. For AR(p) with p greater than 1, we need some tricks to derive covariance.

$$
\begin{align*}
&y_t - \mu = \alpha_1(y_{t-1} - \mu) + \alpha_2(y_{t-2} - \mu) + \varepsilon_t  \\
&(y_t - \mu )(y_{t-j} - \mu )= (\alpha_1(y_{t-1} - \mu))(y_{t-j} - \mu ) + (\alpha_2(y_{t-2} - \mu))(y_{t-j} - \mu ) + (\varepsilon_t)(y_{t-j} - \mu )  \\
&E[(y_t - \mu )(y_{t-j} - \mu )]= \alpha_1E[(y_{t-1} - \mu)(y_{t-j} - \mu )] + \alpha_2E[(y_{t-2} - \mu)(y_{t-j} - \mu )] + E[(\varepsilon_t)(y_{t-j} - \mu )]  \\
&(\therefore) \; \gamma_j = \alpha_1 \gamma_{j-1} + \alpha_2 \gamma_{j-2} \Rightarrow \gamma_j = \alpha_1 L \gamma_{j} + \alpha_2 L^2 \gamma_{j}
\end{align*}
$$

Note that we get another difference equation with $\gamma_j$ and its characteristic equation is same with that of AR(2). The stationary condition for $\gamma_j$ is same with $y_t$.

We assumed the stationary condition for AR(2) is met, hence the covariance term converges.

Before moving on the the variance of $y_t$ in AR(2), let's first check the ACF for AR(2):

$$
\rho_j = \frac{\gamma_j}{\gamma_0} = \alpha_1 \frac{\gamma_{j-1}}{\gamma_0} + \alpha_2 \frac{\gamma_{j-2}}{\gamma_0} = \alpha_1 \rho_1 + \alpha_2 \rho_2 
$$

and

$$
\rho_1 = \alpha_1 \rho_0 + \alpha_2 \rho_{-1} = \alpha_1 + \alpha_2\rho_1 \Rightarrow \rho_1 = \frac{\alpha_1}{1-\alpha_2}
$$

$$
\rho_2 = \alpha_1 \rho_1 + \alpha_2 \rho_0 = \frac{\alpha_1^2}{1-\alpha_2} + \alpha_2
$$

then, you can obtain all $\rho_j$'s.


**(3) Variance**:

Use the same trick for covariance calculation. Then:

$$
\gamma_0 = \alpha_1 \gamma_1 + \alpha_2 \gamma_2 + \sigma^2
$$

After tedious algebraic calculation, we get the term below:

$$
\gamma_0 = \frac{1-\alpha_2}{(1+\alpha_2)[(1+\alpha_2)^2 - \alpha_1^2]}
$$

$\frac{1-\alpha_2}{(1+\alpha_2)[(1+\alpha_2)^2 - \alpha_1^2]}$ is a constant term, which satisfies $< \infty$ (converge)

Thus, AR(2) is stationary process. ($\iff$ ergodic for 2nd moments )

**AR(2) is ergodic for 1st moments:**


$$
\sum_{j=0}^\infty |\gamma_j| = |\gamma_0| + |\gamma_1| + |\gamma_2| + \dots \\
 < \infty \; \text{(per above)}
$$

Hence AR(2) is ergodic for the 1st moments.

### AR(p)

We went over AR(2) in detail. AR(p) is mere generalization of the results. AR(p) is stationary and ergodic for 1st and 2nd moments.

**AR(p) is stationary process ($\lvert \mathbf{\lambda} \rvert < 1$):**

**(1) Mean**:

$$
E[y_t] = \frac{\alpha_0}{1 - \alpha_1 - \alpha_2 - \dots - \alpha_p} = \mu
$$

**(2) Variance**:

$$
\gamma_0 = \alpha_1 \gamma_1 + \alpha_2 \gamma_2 + \cdots + \alpha_p \gamma_p + \sigma^2
$$

**(3) Autocovariance** ($j \neq 0$):

$$
\gamma_j = \alpha_1 \gamma_{j-1} + \alpha_2 \gamma_{j-2} + \cdots + \alpha_p \gamma_{j-p}
$$

**AR(p) solution**

$$
\Phi(L) y_t = \alpha_0 + \varepsilon_t
$$

$$
\begin{align*}
  y_t &= \Phi(L)^{-1} \alpha_0 + \Phi(L)^{-1} \varepsilon_t \\
      &= \frac{\alpha_0}{1 - \alpha_1 - \alpha_2 - \dots - \alpha_p} + \frac{\varepsilon_t}{1 - \alpha_1 L - \alpha_2 L^2 - \dots - \alpha_p L^p}
      &= \mu + \frac{\varepsilon_t}{(1-\lambda_1L)(1-\lambda_2L) \dots (1-\lambda_pL)}
\end{align*}
$$

**ACF for AR(p)**

$$
\rho_j = \alpha_1 \rho_{j-1} + \alpha_2 \rho_{j-2} + \cdots + \alpha_p \rho_{j-p}
$$

## ARMA

$$
\begin{align*}
  y_t = &\alpha_0 + \alpha_1 y_{t-1} + \alpha_2 y_{t-2} + \; \cdots \; + \alpha_q y_{t-p} + \\
        &\varepsilon_t + \beta_1 \varepsilon_{t-1} + \beta_2 \varepsilon_{t-2} + \; \cdots \; + \beta_q \varepsilon_{t-q}
\end{align*}
$$

Solving ARMA: use lag operator. The stationary condition is same with AR(p).

$$
\begin{align*}
  &\alpha(L) y_t = \alpha_0 + \beta(L) \varepsilon_t \\
  &y_t = \frac{\alpha_0}{\alpha(L)} + \frac{\beta(L)}{\alpha(L)} \varepsilon_t \quad \text{(solution)}
\end{align*}
$$

Actually showing the closed form for variance and covariance is very difficult and tedious. The underlying idea is same. Here we assume that the variance and covariance converge under the stationary condition. (It's a known fact.) The expectation is same with that of AR(p). ACF converges to zero as $t$ goes to infinity.

In short, ARMA is stationary and ergodic for 1st and 2nd moments.

## ARIMA

Much of the data we encounter in real life is non-stationary. Therefore, it often needs to undergo preprocessing to restore stationarity before analysis. One of the most common preprocessing techniques is **differencing**. Let’s take a look at how differencing can be used to remove a trend, which is one example of non-stationarity.

### Trend

deterministic trend:
$$
y_t = at + \varepsilon_t
$$

probabilistic trend:
$$
y_t = a\tau_t + \varepsilon_t \text{ where } \tau_t = t + v_t \text{ or } \tau_t = \tau_{t-1} + v
$$

### Effect of Differencing

Case 1. Linear Trend

$$
\begin{align*}
  &y_t = at + \varepsilon_t \\
  &\Delta y_t = y_t - y_{t-1} = a + \varepsilon_t - \varepsilon_{t-1}
\end{align*}
$$

Case 2. Quadiratic Trend

$$
\begin{align*}
  &y_t = at^2 + \varepsilon_t \\
  &\Delta y_t = y_t - y_{t-1} = 2a (t-1) + \varepsilon_t - \varepsilon_{t-1}\\
  &\Delta^2 y_t = \Delta y_t - \Delta y_{t-1} = 3a + \varepsilon_t - 2 \varepsilon_{t-1} + \varepsilon_{t-2}\\
\end{align*}
$$

The same goes for higher order and probabilistic trends.

> Remark: It is advised against to difference more than 2 times due to loss of information. 

### ARIMA(p, d, q) = AR "Integrated" MA

ARIMA can be thought of as ARMA for differenced data. Hence "Integrated" in the middle - it has one more procedure of "patching up" differenced data.

ARIMA has one more parameter standing for the order of differencing. Let's see two examples.

**ARIMA(1, 1, 1)**

$$
\begin{align*}
  &\Delta y_t &= \alpha_0 + \alpha_1 \Delta y_{t-1} + \varepsilon_t + \beta_1 \varepsilon_{t-1} \\
  &\Delta y_t &= \alpha_0 + \alpha_1 L \Delta y_{t} + \varepsilon_t + \beta_1 \varepsilon_{t-1} \\
  &(1- \alpha_1 L)\Delta y_t &= \alpha_0 + (1-\beta_1 L) \varepsilon_t \\
  &(1- \alpha_1 L)(1-L) y_t &= \alpha_0 + (1-\beta_1 L) \varepsilon_t \\
\end{align*}
$$

Note that an additional term $(1-L)$ appeared. This is the integration term added in ARIMA.

**ARIMA(1, 2, 1)**

$$
\begin{align*}
  &\Delta^2 y_t = \alpha_0 + \alpha_1 \Delta^2 y_{t-1} + \varepsilon_t + \beta_1 \varepsilon_{t-1} \\
  & \vdots \\
  &(1- \alpha_1 L)(1-L)^2 y_t = \alpha_0 + (1-\beta_1 L) \varepsilon_t \\
\end{align*}
$$

Generalizing the results:

**ARIMA(1, d, 1)**

$$
(1- \alpha_1 L)(1-L)^d y_t = \alpha_0 + (1-\beta_1 L) \varepsilon_t
$$

**ARIMA(p, d, q)**

$$
\alpha(L)(1-L)^d y_t = \alpha_0 + \beta(L) \varepsilon_t
$$

If $\alpha(L)$ exists,

$$
y_t = \frac{\alpha_0}{\alpha(L)}(1-L)^{-d}  + \frac{\beta(L)}{\alpha(L)}(1-L)^{-d} \varepsilon_t
$$

Stationarity and Ergodicity are covered in sections for MA, AR, and ARMA.

## Impulse Response Analysis for AR(p)

AR(p):

$$
\begin{align*}
  y_t &= \alpha_0 + \alpha_1 y_{t-1} + \alpha_2 y_{t-2} + \; \cdots \; + \alpha_q y_{t-p} + \varepsilon_t \\
      &= f(\varepsilon_t, \varepsilon_{t-1}, \; \cdots, \;)
\end{align*}
$$

Suppose at some point, $t-j$, an impulse occrued. It impacts all subsequent time series values. *Impulse Response Analysis* is conducted to analyze this impact in terms of (1) how long does the impact last? and (2) how strong is the impact? In other words, it is an analysis for the impact of exogenous impulse not explained by autoregression.

### Impulse Response Function

For stationary time series: 

$$
\Psi(j) = \frac{\partial y_t}{\partial \varepsilon{t-j}} = \frac{\partial y_{t+j}}{\partial \varepsilon{t}}
$$

#### Case: AR(1)

$$
y_t = \alpha_0 + \alpha_1 y_{t-1} + \varepsilon_t = \frac{\alpha_0}{1-\alpha_0} + \sum_{j=0}^\infty \alpha_{1}^{j} \varepsilon_{t-j}
$$

Therefore;

$$
\Psi(j) = \frac{\partial y_t}{\partial \varepsilon{t-j}} = \alpha_1^j
$$

#### Case: AR(2) ...?

$$
y_t = \frac{\alpha_0}{1-\alpha_1 - \alpha_2} + \frac{\varepsilon_t}{1-\alpha_1 L - \alpha_2 L^2}
$$

We can see getting Wold Representation for AR(p) ($p \geq 2$) is diffeicult. We use math trick called Space State Form.

#### Impulse Response Function for AR(p): Wold Representation in Space State Form


We remove the constant term since it is irrelevant in impulse response analysis.

**Example: AR(2)**

$$
y_t = \alpha_1 y_{t-1} + \alpha_2 y_{t-2} + \varepsilon_t
$$

State-space representation:
$$
\begin{bmatrix}
y_t \\
y_{t-1}
\end{bmatrix}
=
\begin{bmatrix}
\alpha_1 & \alpha_2 \\
1 & 0
\end{bmatrix}
\begin{bmatrix}
y_{t-1} \\
y_{t-2}
\end{bmatrix}
+
\begin{bmatrix}
1 \\
0
\end{bmatrix} \varepsilon_t
$$

---

**General Case: AR(p)**

General AR(p) model:
$$
y_t = \alpha_1 y_{t-1} + \dots + \alpha_p y_{t-p} + \varepsilon_t
$$

State-space form:
$$
\mathbf{y}_t =
\begin{bmatrix}
\alpha_1 & \alpha_2 & \dots & \alpha_p \\
1 & 0 & \dots & 0 \\
0 & 1 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & 1 & 0
\end{bmatrix}
\mathbf{y}_{t-1}
+
\begin{bmatrix}
1 \\
0 \\
0 \\
\vdots \\
0
\end{bmatrix} \varepsilon_t
$$

Let:
$$
\mathbf{y}_t = F \mathbf{y}_{t-1} + \eta_t
$$
where $\eta_t = [\varepsilon_t, 0, \dots, 0]^\top$.

**Wold Representation**

From:
$$
(I - F L) \mathbf{y}_t = \eta_t
$$
we have:
$$
\mathbf{y}_t = (I - F L)^{-1} \eta_t
$$

Expanding:
$$
(I - F L)^{-1} = I + F L + F^2 L^2 + \dots
$$

Thus:
$$
\mathbf{y}_t = \eta_t + F \eta_{t-1} + F^2 \eta_{t-2} + \dots
$$

The **Wold representation** is:
$$
y_t = \varepsilon_t + f_{1,(1,1)} \varepsilon_{t-1} + f_{2,(1,1)} \varepsilon_{t-2} + \dots
$$
where $f_{j,(1,1)}$ is the $(1,1)$ element of $F^j$.

---

**Example: AR(2)**

Let:
$$
F =
\begin{bmatrix}
\alpha_1 & \alpha_2 \\
1 & 0
\end{bmatrix}
$$

Then:
$$
F^2 =
\begin{bmatrix}
\alpha_1^2 + \alpha_2 & \alpha_1 \alpha_2 \\
\alpha_1 & \alpha_2
\end{bmatrix}
$$

The impulse response at lag $j$ is:
$$
\psi_j = f_{j,(1,1)}
$$


#### Using Eigen Decomposition

The challenging part of computation would be getting $F^j$. For this we use Eigen Decomposition.

Instead, use the eigen decomposition:
$$
F = C \Lambda C^{-1}
$$
where $\Lambda$ is diagonal with eigenvalues $\lambda_i$.

Then:
$$
F^j = C \Lambda^j C^{-1}
$$

The $(1,1)$ element is:
$$
f_{j,(1,1)} = \sum_{i=1}^p \lambda_i^j \, C_{1i} \, (C^{-1})_{i1}
$$

Interpretation:
- Weighted average of eigenvalues raised to the $j$-th power
- Weights: $C_{1i} (C^{-1})_{i1}$
- $\sum_{i=1}^p C_{1i} (C^{-1})_{i1} = 1$


#### Final Form of Impulse Response Function

$$
y_t = \varepsilon_t + \psi_1 \varepsilon_{t-1} + \psi_2 \varepsilon_{t-2} + \dots
$$
where:
$$
\psi_j = f_{j,(1,1)} = \sum_{i=1}^p \lambda_i^j \, C_{1i} \, (C^{-1})_{i1}
$$

#### Last Remark

So how do we interpret impulse response function?

1. How strong is the impact of the impulse j period ago? -> check $\Psi(j)$
2. How long does that impact last? -> check the shape of $\Psi(j)$ (for stationary process, it decreases over time.)

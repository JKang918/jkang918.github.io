---
title: Stochastic Process > 8. Exponential Distribution
date: 2025-07-13 #HH:MM:SS +/-TTTT
categories: [Stochastic Process, General (ISYE6650)]
tags: [stochastic process, georgia tech, dtmc, semi-dtmc]     # TAG names should always be lowercase
author: <author_id>                     # for single entry
description: Georgia Tech ISYE6650 Fall 2024 LC notes
toc: true
math: true
mermaid: true
comments: true
---

In the previous posts, we focused on **Discrete-Time Markov Chains (DTMC)**.  
Now, we turn our attention to one of the most important continuous-time stochastic processes: the **Continuous-Time Markov Chain (CTMC)**.  
Before diving into CTMCs, however, itâ€™s essential to understand the **Exponential distribution** and the properties of **Exponential random variables**, which form the foundation for modeling time in continuous settings.


## Basic Information: Exponential Distribution

Let $X \sim \text{Exp}(\lambda)$, where $\lambda > 0$ is the rate parameter.

- **PDF**:
  $$
  f(x) = 
  \begin{cases}
    \lambda e^{-\lambda x}, & x \geq 0 \\
    0, & x < 0
  \end{cases}
  $$

- **CDF**:
  $$
  F(x) = 
  \begin{cases}
    1 - e^{-\lambda x}, & x \geq 0 \\
    0, & x < 0
  \end{cases}
  $$

- **Mean**: $\mathbb{E}[X] = \dfrac{1}{\lambda}$  
- **Variance**: $\mathrm{Var}(X) = \dfrac{1}{\lambda^2}$

- **MGF**:
  $$
  \phi(t) = 
  \begin{cases}
    \dfrac{\lambda}{\lambda - t}, & t < \lambda \\
    \infty, & t \geq \lambda
  \end{cases}
  $$


## Properties of the Exponential Distribution

### 1. Memoryless Property

> This is the most important property in modelling

Let $s, t > 0$. Then:

$$
P(X > s + t \mid X > s) = P(X > t)
$$


**proof**

$$
\begin{aligned}
P(X > s + t \mid X > s) &= \frac{P(X > s + t)}{P(X > s)} \\
&= \frac{e^{-\lambda(s + t)}}{e^{-\lambda s}} = e^{-\lambda t}
= P(X > t)
\end{aligned}
$$


### 2. Minimum of Exponential Variables

Let $X_1, X_2, \dots, X_n$ be independent where $X_i \sim \text{Exp}(\lambda_i)$, and let:

$$
Y = \min\{X_1, X_2, \dots, X_n\}
$$

Then:

- CDF of $Y$:
  
  $$
  \begin{aligned}
  F_Y(y) = P(Y \leq y) \\
        &= P \left( \min_{1 \leq i \leq n} X_i \leq y \right)   \\
        &= 1 - P \left( \min_{1 \leq i \leq n} X_i > y \right) \quad (\text{that is, all X's are greater than y})  \\
        &= 1 - P \left( X_1, X_2, \cdots X_n > y \right)   \\
        &= 1 - P \left( X_1 > y \right)P \left( X_2 > y \right) \cdots P \left( X_n > y \right)   \\
        &= 1 - \prod_{i=1}^n P(X_i > y) \\
        &= 1 - \prod_{i=1}^n \left[ 1 - P(X_i \leq y) \right] \\
        & = 1 - \prod_{i=1}^n e^{-\lambda_i y} = 1 - e^{-y \sum_{i=1}^n \lambda_i} \; (y \geq 0)
  \end{aligned}
  $$

- So:
  $$
  Y \sim \text{Exp}(\lambda_1 + \cdots + \lambda_n)
  $$


### 3. Sum of Exponential Variables

Let $X_1, \dots, X_n \overset{\text{i.i.d.}}{\sim} \text{Exp}(\lambda)$, and define:

$$
Z = \sum_{i=1}^n X_i
$$

> We covered this in previous post: [link](https://jkang918.github.io/posts/Post1/)

Then $Z$ has an Erlang distribution:

$$
Z \sim \text{Erlang}(n, \lambda)
\quad \text{which is a special case of } \text{Gamma}(n, \lambda)
$$

**PDF**:
$$
f_Z(z) = 
\begin{cases}
\dfrac{\lambda^n z^{n-1} e^{-\lambda z}}{(n-1)!}, & z \geq 0 \\
0, & z < 0
\end{cases}
$$

**Expected Value and Variance**:

- $\mathbb{E}[Z] = \dfrac{n}{\lambda}$
- $\mathrm{Var}(Z) = \dfrac{n}{\lambda^2}$

**MGF**:
$$
\phi_Z(t) = 
\begin{cases}
\left( \dfrac{\lambda}{\lambda - t} \right)^n, & t < \lambda \\
\infty, & t \geq \lambda
\end{cases}
$$

> for CDF, we will get back to it later.

### 4. Probability Comparisons Between Two Exponentials

Let $X \sim \text{Exp}(\lambda)$ and $Y \sim \text{Exp}(\mu)$, where $X$ and $Y$ are independent.

#### Key Probabilities:

- $P(X < Y) = \dfrac{\lambda}{\lambda + \mu}$
- $P(X = Y) = 0$ (since continuous RVs)
- $P(X > Y) = \dfrac{\mu}{\lambda + \mu}$

#### Derivation of $P(X < Y)$:

$$
\begin{aligned}
P(X < Y) &= \int_0^\infty P(X < y) \cdot f_Y(y) \, dy \\
&= \int_0^\infty \left( 1 - e^{-\lambda y} \right) \cdot \mu e^{-\mu y} \, dy \\
&= \mu \int_0^\infty \left( e^{-\mu y} - e^{-(\lambda + \mu) y} \right) dy \\
&= \mu \left( \frac{1}{\mu} - \frac{1}{\lambda + \mu} \right) \\
&= \frac{\lambda}{\lambda + \mu}
\end{aligned}
$$

Let's think about intutive interpretation. Suppose the event is machine breakdown and $X$ represents the time it takes for machine $x$ to breakdown and $Y$ represents that of machine $y$. Assume $\lambda$ is greater than $\mu$. This means $x$ breaks down more frequently as $\lambda$ and $\mu$ represent rates. Then it would be more likely for the time (= $X$) it takes for $x$ to break down **before** $y$ to break down would be higher than the other way around. More simply, $\frac{\lambda}{\lambda + \mu} >  \frac{\mu}{\lambda + \mu}$ .


---

**Credit**\
All contents in this post are from a digitized version of my own lecture notes taken during *ISYE 6650: Probabilistic Models and Their Applications* (Fall 2024, Georgia Tech, [**Prof. Sigrun Andradottir** (Georgia Tech Link)](https://www.isye.gatech.edu/users/sigrun-andradottir)).
Full credit for the course materials and original explanations belongs to the professor.

---
title: Econometrics > 3. Asymptotic Properties
date: 2025-08-29 #HH:MM:SS +/-TTTT
categories: [Econometrics, OLS and Others]
tags: [econometrics, asymptotic_property, big_o, little_o, clt, cmt, lln]     # TAG names should always be lowercase
author: <author_id>                     # for single entry
description: Theoratical Econometrics
toc: true
math: true
mermaid: true
comments: true
---

Starting with this post, I will go over remedial methods to OLS when each assumption is broken.

We discussed A5. Normality as a necessary condition that enables t-test and F-test. If normality condition is not met, t-test and F-test become invalid.

However, thanks to the **Asymptotic Properties**, we will see as long as the sample size is large enough, t-test and F-test can be run and used even when the normality condition is not met.

First, we are going to study the formal definitions of relevant fundamental concepts.
Next, with those, we are going to come up with an alternative assumption to A5., A5 a.
Lastly, we will see, under A5 a., t-test and F-test become valid tests.


## Definitions

### Consistency

The followings are all equal (T.F.A.E., hereafter):

$$
\begin{align*}
  & \hat{\theta} \xrightarrow{p} \theta \text{ as } n \rightarrow \infty \\
  & \mathrm{plim}_{x\to\infty} (\theta) = \theta \\
  & \varepsilon > 0, \; P(|\hat{\theta} - \theta| > \varepsilon) \rightarrow 0 \text{ as } n \rightarrow \infty
\end{align*}
$$

If an estimator, $\hat{\theta}$ converge in probability to the true parameter, $\theta$, we say the estimater is "consistent".

### Little-o, Big-O

Little-o means "is ultimately smaller than".
Big-O means "is of the same order as".\
Thereby, little-o is closely related to convergence and big-O to boundedness.\
(I borrowed the expressions in "..." from [here](https://www.stat.cmu.edu/~cshalizi/uADA/13/lectures/app-b.pdf) - Carnegie Mellon University Stat Course material, 2013)

Given:

$$
\begin{align*}
\{\mathbf{x_n}\}: \text{ sequence of nonrandom vectors} \\
\{a_n\}: \text{ sequence of nonrandom scalars}  
\end{align*}
$$

(1) little-o

(T.F.A.E.)

- $\lVert \mathbf{x_n} \rVert = \mathrm{o}(a_n)$
- $\frac{\lVert \mathbf{x_n} \rVert}{a_n} = \mathrm{o}(1)$
- $\frac{\lVert \mathbf{x_n} \rVert}{a_n} \rightarrow 0 \text{ as } n \rightarrow \infty$

(2) Big-O

(T.F.A.E.)

- $\lVert \mathbf{x_n} \rVert = \mathrm{O}(a_n)$
- $\frac{\lVert \mathbf{x_n} \rVert}{a_n} = \mathrm{O}(1)$
- ${\scriptsize \exists} M < \infty, \text{ s.t. } \lVert \mathbf{x_n} \rVert \leq M a_n, \; {\scriptsize \forall} n \in \mathbb{N}$


### Little-o and Big-O in Probability

We can expand the concepts of little-o and big-O into probabilistic world.

Given:

$$
\begin{align*}
\{ X_n \}: \text{ sequence of random vectors} \\
\{ Y_n \}: \text{ sequence of random vectors} \\ 
\end{align*}
$$

(1) little-o in probability (convergence in probability)

(T.F.A.E.)

- $X_n = \mathrm{o_p}(Y_n)$
- $\frac{X_n}{\lVert Y_n \rVert} \xrightarrow{p} 0 \text{ as } n \rightarrow 0$
- $\mathrm{P}(\lVert X_n \rVert \geq \varepsilon \lVert Y_n \rVert) \rightarrow 0 \text{ for every } \varepsilon > 0$ 



(2) Big-O in probability (boundedness in probability)

(T.F.A.E.)

- $X_n = \mathrm{O_p}(Y_n)$
- ${\scriptsize \exists} M < \infty, \text{ s.t. }  \mathrm{P}(\lVert X_n \rVert \leq M \lVert Y_n \rVert) > 1 - \varepsilon \text{ for every } \varepsilon > 0$ 

\
**Application of $\mathrm{O_p}(\cdot)$ and $\mathrm{o_p}(\cdot)$**

Let $X_n = \mathrm{O_p}(1)$ and $Y_n = \mathrm{o_p}(1)$. \
(informal interpretation: $X_n$ converges to a constant and $Y_n$ is bounded by a constant.)

(a) $X_n * Y_n = \mathrm{O_p}(1) * \mathrm{o_p}(1) = \mathrm{o_p}(1)$ \
(b) $X_n + Y_n = \mathrm{O_p}(1) + \mathrm{o_p}(1) = \mathrm{O_p}(1)$\
(c) $X_n \xrightarrow{p} c \Rightarrow X_n = c + \mathrm{o_p}(1)$\
(d) If $X_n \xrightarrow{p} c$ and $Y_n \xrightarrow{d} Y$ then $X_n * Y_n = (c + \mathrm{o_p}(1)) * Y_n \xrightarrow{d} c * Y_n$

With the informal interpretation above in mind, all of these can be understood accordingly. For more thorough understanding, taking analysis or real analysis class would be helpful.


### LLN (Law of Large Numbers)  

$Y_i \sim \text{iid}, \; \forall i = 1, \; 2, \;, \cdots \; n \rightarrow$ all random variables are iid. (This assumption can be relaxed a bit)

$\mu = \mathrm{E}[Y_i], \; \forall i$: population mean

$$
\frac{1}{n}\sum_{i=1}^n Y_i \xrightarrow{p} \mathrm{E}[Y_i] = \mu \quad \text{as } n \to \infty 
$$

As the sample size grows, the sample mean converges in probability to the population mean.

### CLT (Central Limit Theorem)

$$
\frac{1}{\sqrt{n}} \sum_{i=1}^n (Y_i - \mu) \xrightarrow{d} \mathcal{N}(0, \sigma^2) \quad \text{as } n \to \infty
$$

   - $\mathrm{E}[Y_i] = \mu$  
   - $\mathrm{Var}(Y_i) = \sigma^2$

As the sample size grows, the sample mean converges in distribution to normal distribution.
Refer to the heuristic proof of CLT if you are interested in [this post](https://jkang918.github.io/posts/Post10/).

### CMT (Continuous Mapping Theorem)  

If $\hat{\theta}_n \xrightarrow{p} \theta$ (a sequence of random variable converges in probability), then  

$$
f(\hat{\theta}_n) \xrightarrow{p} f(\theta) \quad \text{as } n \to \infty
$$

where $f$ is continuous.

Applying a continuous function to a convergent random variable preserves the convergence.


We are done with the necessary definitions by now.

## A5.a. LLN and CLT Hold. Therefore we have the followings

(a)  
$$
\frac{1}{n} X^\top X = \frac{1}{n}\sum_{i=1}^n \mathbf{x_i}^\top \mathbf{x_i} \xrightarrow{p} \mathrm{E}[\mathbf{x_i}^\top \mathbf{x_i}] = Q_{xx}
$$

(b)  
$$
\frac{1}{n} X^\top \mathbf{u} = \frac{1}{n}\sum_{i=1}^n \mathbf{x_i}^\top u_i \xrightarrow{p} \mathrm{E}[\mathbf{x_i}^\top u_i]
$$

(c)  
$$
\frac{1}{\sqrt{n}} X^\top u = \frac{1}{\sqrt{n}}\sum_{i=1}^n \big( \mathbf{x_i} u_i - \mathrm{E}[\mathbf{x_i}^\top u_i] \big) \xrightarrow{d} \mathcal{N}(0, \mathrm{Var}(\mathbf{x_i}^\top u_i)),
$$

if $\mathrm{E}[\mathbf{x_i}^\top u_i] = 0$

(d)  
$$
\frac{1}{n} \mathbf{u}^\top \mathbf{u} = \frac{1}{n}\sum_{i=1}^n u_i^2 \xrightarrow{p} \mathrm{E}[u_i^2] = \sigma^2
$$

($\mathbf{x_i}$: $i$th row vector of $X$)


(a), (b), (d) are the results of LLN, while (c) is that of CLT.

**Note 1.**

Is $\mathrm{E}[\mathbf{x_i}^\top u_i] = 0$ ? Note that this is exogeneity condition, which holds true when A3. Zero Conditiona Mean is met. We assume A3. is met so (c) is satisfied.

**Note 2.**

From (c), we can further move onto $\mathcal{N}(0, \mathrm{Var}(\mathbf{x_i}^\top u_i)) = \mathcal{N}(0, \sigma^2 Q_{xx})$

pf)

$$
\begin{align*}
\mathrm{Var}(\mathbf{x_i}^\top u_i) &= \mathrm{E}[\{ \mathbf{x_i}^\top u_i - \mathrm{E}[\mathbf{x_i}^\top u_i] \} \{ \mathbf{x_i}^\top u_i - \mathrm{E}[\mathbf{x_i}^\top u_i] \}^\top] \\
&= \mathrm{E}[(\mathbf{x_i}^\top u_i) (\mathbf{x_i}^\top u_i)^\top] \\
&= \mathrm{E}[\mathbf{x_i}^\top u_i u_i^\top \mathbf{x_i}] \\
&= \mathrm{E}[\mathbf{x_i}^\top (u_i)^2 \mathbf{x_i}] \\
&= \mathrm{E}[\mathbf{x_i}^\top \mathbf{x_i}] \mathrm{E}[u_i^2] \text{ (exogeneity)} \\
&= \sigma^2 \mathrm{E}[\mathbf{x_i}^\top \mathbf{x_i}] \\
&= \sigma^2 Q_{xx} \qquad \square 
\end{align*}
$$


## Theorems for t-test and F-test

Now with the A5.a., let's see why t-test and F-test are possible without the Normality assumption.

### Theorem 1. Consistency of OLS

Under assumptions: A1, A2, A3, and A5.a,

$$
\hat{\mathbf{\beta}} \xrightarrow{p} \beta
$$

**proof**

$$
\begin{align*}
\hat{\beta} &= (X^\top X)^{-1} X^\top  \mathbf{u}  + \beta \\
            &= (\frac{1}{n} X^\top X)^{-1} (\frac{1}{n} X^\top X) + \beta \\
            & \xrightarrow{p} \beta + \left(  \mathrm{E}[\mathbf{x_i}^\top \mathbf{x_i}] \right)^{-1} \left(  \mathrm{E}[\mathbf{x_i}^\top \mathbf{x_i}] \right) \\
            &= \beta \qquad \square  
\end{align*}
$$

### Theorem 2. Asymptotic Distribution of OLS

Under assumptions: A1, A2, A3, and A5.a,

$$
\sqrt{n}(\hat{\mathbf{\beta}} - \beta) \xrightarrow{d} \mathcal{N}(0, \sigma^2 \mathrm{E}(\mathbf{x_i}^\top \mathbf{x_i})^{-1})
$$

**proof**

$$
\hat{\beta} = (X^\top X)^{-1} X^\top \mathbf{u} + \beta
$$
$$
\hat{\beta} - \beta = (X^\top X)^{-1} X^\top \mathbf{u}
$$

$$
\begin{align*}
\sqrt{n}(\hat{\beta} - \beta) &= (\frac{1}{n} X^\top X)^{-1} \frac{1}{\sqrt{n}} X^\top \mathbf{u} \\
                              &\xrightarrow{d} \mathrm{E}(\mathbf{x_i}^\top \mathbf{x_i})^{-1} \mathcal{N}(0, \sigma^2 Q_{xx})
                              = \mathcal{N}(0, \sigma^2 Q_{xx}^{-1}) \qquad \square
\end{align*}
$$

> Note that $Q_{xx}$ is population covariance matrix 

$$ \mathrm{Var}(\hat{\beta}) = \sigma^2 (X^\top X)^{-1} \approx \frac{\sigma^2}{n} Q_{xx}^{-1} $$

### Theorem 3. Consistent Estimator of $\sigma^2$

Under assumptions: A1, A2, A3, A4, and A5.a,

Given $\hat{\sigma}^2 = \dfrac{\hat{\mathbf{u}}^\top \hat{\mathbf{u}}}{n-k-1}$,

$$
\hat{\mathbf{\sigma}} \xrightarrow{p} \sigma
$$

**proof**

Recall:

$$
\begin{align*}
  &\hat{\mathbf{u}} = M_x \mathbf{y} = M_x \mathbf{u} \qquad M_x \text{: residual maker matrix} \\
  &\hat{\sigma}^2 = \dfrac{\hat{\mathbf{u}}^\top \hat{\mathbf{u}}}{n-k-1}
\end{align*}
$$

Then:

$$
\begin{align*}
\hat{\mathbf{u}}^\top \hat{\mathbf{u}} &= \mathbf{u}^\top M_x^\top M_x \mathbf{u} \\
            &= \mathbf{u}^\top M \mathbf{u} \\
            &= \mathbf{u}^\top \left[ I - X(X^\top X)^{-1}X^\top \right] \mathbf{u}  \\
            &= \mathbf{u}^\top \mathbf{u} - \mathbf{u}^\top X(X^\top X)^{-1}X^\top \mathbf{u}
\end{align*}
$$

Therefore:

$$
\begin{align*}
\frac{1}{n} \hat{\mathbf{u}}^\top \hat{\mathbf{u}} &= \frac{1}{n} \mathbf{u}^\top \mathbf{u} - \frac{1}{n} \mathbf{u}^\top X(X^\top X)^{-1}X^\top \mathbf{u}\\
            &= \frac{1}{n} \mathbf{u}^\top \mathbf{u} - \frac{1}{n} \mathbf{u}^\top X(\frac{1}{n} X^\top X)^{-1} \frac{1}{n} X^\top \mathbf{u} \\
            &\xrightarrow{p} \mathrm{E}[u_i^\top u_i] = \sigma^2 \qquad \square
\end{align*}
$$


**Conclusion**

By theorem 1 and 3, we can be confident that as the sample size grows our estimators approach the true parameter values.

By theorem 2, we can see $\hat{\beta}$ *asymptotically* follows normal distribution, thereby t-test and F-test results can be trusted regardless of A5. Normality. How asympotic normality makes valid t-test and F-test is identical to that of actual normality case, so refer to [the previous post](https://jkang918.github.io/posts/Post2/), if you would like to go through it again. 

**We can conduct t-test and F-test. For a large sample, the inference precedure is valid.**

---

**Credit**\
All contents in this post are from a digitized version of my own lecture notes taken during *Econometric Theory I* (Fall 2019, Sungkyunkwan University, [**Prof. Heejoon Han** (Personal Homepage Link)](https://sites.google.com/site/heejoonecon/)) and my own self-study materials & other sources. For specific references worth noting, I included in each relevant part.

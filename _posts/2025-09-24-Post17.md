---
title: Econometrics > 4. Heteroskedasticity
date: 2025-09-24 #HH:MM:SS +/-TTTT
categories: [Econometrics, OLS and Others]
tags: [econometrics, heteroskedasticity, homoskedasticity, hac, white, gls, wls]     # TAG names should always be lowercase
author: <author_id>                     # for single entry
description: Theoretica1 Econometrics
toc: true
math: true
mermaid: true
comments: true
---

We learned in [the previous post](https://jkang918.github.io/posts/Post16/) that even when **A5. Normality** condition is not met, the results for t-tests and F-tests are *asymptotically* valid.

In this post, we take a look at **A4. Homoskedasticity & No AutoCorrelation** condition. We know that this assumption is critical in determining that the OLS estimator is BLUE - Best Linear Unbiased estimator. Before we delve into remedial methods in the case of heteroskedasticity and/or autocorrelation, first let's see how to detect them. In this post and following posts, I will try to go through statistical tests to detect heteroskedasticity and/or autocorrelation and the corresponding remedial methods.

## Overview

Assumption A4 is composed of two parts:

$$
\mathrm{Var}(\mathbf{u} | X) = \sigma^2I = 
\begin{bmatrix}
 \sigma_1^2 &  & & \\
            & \sigma_2^2 & & \\
            &            & \ddots  \\
            &            &        &\sigma_n^2\\
\end{bmatrix}
$$


1. **Homoskedasticity**: The variance of the error terms is constant for given $X$:
   
   $$
   \mathrm{Var}[u_i \mid X] = \sigma^2, \qquad \forall i
   $$

2. **No Autocorrelation**: The error terms are uncorrelated with each other (for $i \neq j$):
   
   $$
   \mathrm{Cov}(u_i, u_j \mid X) = 0, \qquad \forall i \neq j
   $$

- if diagonal elements are not constants $\Rightarrow$ **Heteroskedasticity** 
- if offdiagonal elements are not zeros $\Rightarrow$ **AutoCorrelation**

In this post, we focus on "**Heteroskedasticity**".

As remedial methods for "**Heteroskedasticity**", we are going to cover two methods:

(1) Robust Standard Error
(2) GLS (Generalized Least Square) & Cochrane-Orcutt 

## Effects of Heteroskedasticity

$$
\begin{align*}
\mathrm{Var}[\hat{\beta} \mid X] 
&= \mathrm{Var}\left[\,\beta + (X^{\top} X)^{-1} X^{\top} \mathbf{u} \,\middle|\, X \right] \\
&= \mathrm{Var}\left[ (X^{\top} X)^{-1} X^{\top} \mathbf{u} \mid X \right] \\
&= (X^{\top} X)^{-1} X^{\top}\, \mathrm{Var}[\mathbf{u} \mid X]\, X (X^{\top} X)^{-1} \\
&= (X^{\top} X)^{-1} X^{\top}\, \Omega\, X (X^{\top} X)^{-1}
\end{align*}
$$

Since $\mathrm{Var}[\mathbf{u} \mid X] = \Omega \neq \sigma^2 I$, $\mathrm{Var}[\hat{\beta} \mid X]$ no longer collapses into $\sigma^2 (X^\top X)^{-1}$. Thereby, inferences made based on $\sigma^2 (X^\top X)^{-1}$ becomes invalid and OLS estimator no longer is BLUE (Best Linear Unbiased Estimator).  


Let's end this section with a practical example of heteroskedasticity.

$$
\begin{align*}
\text{savings}_i = \beta_0 + \beta_1 \text{income}_i + u_i \\
\mathrm{Var}(\mathbf{u}|\text{income}_i) = \sigma^2 \text{income}_i \Rightarrow \text{heteroskedastic}
\end{align*}
$$

Let $\beta_0 = 0$, $\beta_1 = 0.5$, which are OLS estimates.

For observation 2, suppose we witnessed that $\text{income}_2 = 1$ mil, so the predicted value for savings for this person 2 is 0.5 mil. Let's consider two most extreme cases.

(1) Actual savings value is $0$ $\Rightarrow \; u_2 = -0.5$ mil
(2) Actual savings value is $1$ mil $\Rightarrow \; u_2 = 0.5$ mil

So for person 2, variance is $0.25$.

Another case. for observation 3, suppose we witnessed that $\text{income}_3 = 10$ mil, so the predicted value for savings for this person 3 is 0.5 mil. Let's consider two most extreme cases.

(1) Actual savings value is $0$ $\Rightarrow \; u_2 = -5$ mil
(2) Actual savings value is $10$ mil $\Rightarrow \; u_2 = 5$ mil

So for person 3, variance is $25$.


## Test for Heteroskedasticity


### 1. Breusch–Pagan (BP) Test

$\mathrm{Var}[u_i \mid X] = \sigma^2, \qquad \forall i$ (homoskedasticity) suggests that $\mathbf{u}$ and $X$ are not related. So, using $\hat{\mathbf{u}}$ as a proxy for $\mathbf{u}$, if there is an observed relationship between $\hat{\mathbf{u}}$ and $X$, we can conclude homoskedasticy assumption is broken.



$$
\text{BP Test}
$$


$$
\hat{u}_i^2 = \delta_0 + \delta_1 x_{1i} + \delta_2 x_{2i} + \cdots + \delta_k x_{ki} + e_i, \; \forall i = 1,\: 2,\: \cdots,\:  n
$$

$$
\begin{align*}
&\text{H}_0: \; \delta_1 = \delta_2 = \; \cdots \; = \delta_k = 0 \\
&\text{H}_1: \; \sim \text{H}_0 \\
\end{align*}
$$

- Run OLS regression of residuals on the independent variables,
- Use $R^2$ of the regression, 
  $$\hat{R}^2_{\hat{u}^2_i}$$
  
  
  , and compute the Lagrange Multiplier statistic, which has asymptotic distribution: $\text{LM} = n\hat{R}^2_{\hat{u}^2_i} \xrightarrow{d} \chi_k^2$  

- Test result: $\chi^2$ value large enough to reject $\text{H}_0 \Rightarrow$ heteroskedastistic

One obvious downside easily observable is that nonlinear relationships are not captured by $\delta$'s.

### 2. White Test

The idea is same as BP test. To capture non-linear relationships, in the hypothesis model specification, cross-terms and square terms are included. Optionally, softwares cross-terms, if needed, can be excluded. The rest of test procedures are identical. 

$$
\text{White Test}
$$

$$
\hat{u}_i^2 = \delta_0 + (\delta_1 x_{1i} +  \cdots + \delta_k x_{ki}) + (\delta_{k+1} x_{1i}^2 + \cdots + \delta_{2k} x_{ki}^2) + [\text{cross-terms}] +  e_i, \; \forall i
$$

$$
\begin{align*}
&\text{H}_0: \; \delta_1 = \delta_2 = \; \cdots \; = \delta_p = 0 \\
&\text{H}_1: \; \sim \text{H}_0 \\
\end{align*}
$$

- $\text{LM} = n\hat{R}^2_{\hat{u}^2_i} \xrightarrow{d} \chi_p^2$
- Test result: $\chi^2$ value large enough to reject $\text{H}_0 \Rightarrow$ heteroskedastic

> Remark. White test can be thought of as extended version of BP test.


## Solutions for Heteroskedasticity

### 1. Robust Standard Error

Even with the presence of heteroskedasticity, as long as **A3. Zero Conditional Mean** is met, the estimator is unbiased. The problem caused by heteroskedasticity is about difficulties it places on statistical inference for coefficients (i.e., estimators). 

$$
\begin{align*}
\mathrm{Var}[\hat{\beta} \mid X] &= (X^{\top} X)^{-1} X^{\top}\, \Omega\, X (X^{\top} X)^{-1}
\end{align*}
$$

1. homoskedasticity: $\Omega = \sigma^2 I \; \Rightarrow \; \mathrm{Var}[\hat{\beta} \mid X] = \sigma^2 (X^\top X)^{-1}$
2. heteroskedasticity: $\Omega = \mathrm{diag}(\sigma_1,\; \sigma_2,\; \cdots,\; \sigma_n) \neq \sigma^2 I \Rightarrow \; \mathrm{Var}[\hat{\beta} \mid X] \neq \sigma^2 (X^\top X)^{-1}$

So, we need to correct for the standard errors used for t-test and F-test, which are based on $\mathrm{Var}[\hat{\beta} \mid X] = \sigma^2 (X^\top X)^{-1}$.

#### 1.1 White's Proposal

Assumption: no autocorrelation $\Rightarrow \; \Omega = \mathrm{diag}(\sigma_1^2,\; \sigma_2^2,\; \cdots,\; \sigma_n^2)$

The true variance of interest is: 

$$
\begin{align*}
\mathrm{Var}[\hat{\beta} \mid X] &= (X^{\top} X)^{-1} X^{\top}\, \Omega\, X (X^{\top} X)^{-1} \\
&= (X^{\top} X)^{-1} \left( \sum_{i=1}^n \sigma_i^2 \mathbf{x}_i \mathbf{x}_i^\top \mathbf{x} \right) (X^{\top} X)^{-1}
\end{align*}
$$

**White's Proposal**

$$
\begin{align*}
\widehat{
    \mathrm{Var}[\hat{\beta} \mid X]_{\text{White}}
} &= (X^{\top} X)^{-1} X^{\top}\, \widehat{\Omega}\, X (X^{\top} X)^{-1} \\
&= (X^{\top} X)^{-1} \left( \sum_{i=1}^n \hat{u}_i^2 \mathbf{x}_i \mathbf{x}_i^\top \right) (X^{\top} X)^{-1}
\end{align*}
$$

That is,


$$
\hat{\Omega} = 
\begin{bmatrix}
 \hat{u}_1^2 &  & & \\
            & \hat{u}_2^2 & & \\
            &            & \ddots  \\
            &            &        &\hat{u}_n^2\\
\end{bmatrix}
$$

Then, 

$$
\widehat{
    \mathrm{Var}[\hat{\beta} \mid X]_{\text{White}}
} \xrightarrow{p} \mathrm{Var}[\hat{\beta} \mid X]
$$

by LLN, [Law of Large Number](http://127.0.0.1:4000/posts/Post16/#a5a-lln-and-clt-hold-therefore-we-have-the-followings).

**Conlcusion.** 

- Use residuals from observation use that for estimating variance. Use this variance for computing standard errors for t-test and F-test. It is unnecessary to know the true form of heteroskedasticity.

- Valid for Large samples (because LLN has to hold.)

- OLS estimator is still inefficient; A4. needs to hold for OLS to be BLUE.


#### 1.2 Newey-West Heteroskedasticity and Autocorrelation Consistent (HAC) covariance matrix estimator

Let,

$$
S_0 = \sum_{i=1}^n \hat{u}_i^2  \mathbf{x}_i\mathbf{x}^\top_i
$$

and this is the value used for $X^{\top}\, \widehat{\Omega}\, X$ for White's proposal.

In Newey-West HAC, use the following value instead:

$$
X^{\top}\, \widehat{\Omega}\, X = S_0 + \sum_{\ell=1}^L \sum_{i=\ell+1}^n 
\left(1 - \frac{\ell}{L+1}\right) 
\hat{u}_i \hat{u}_{i-\ell} 
\left(\mathbf{x}_i \mathbf{x}_{i-\ell}^\top + \mathbf{x}_{i-\ell} \mathbf{x}_i^\top \right)
$$

The advantage of the Newey-West HAC estimator is that it provides consistent standard errors even when both heteroskedasticity and autocorrelation are present in the error terms.
While White’s robust standard errors **only** correct for heteroskedasticity (assuming no autocorrelation), the Newey-West estimator is robust to both issues.

Other implications that OLS estimator is not BLUE and others in the case of White's Proposal stay same with Newey-West HAC.

> Remark. $L$ determines up to which **lag** the autocorrelation in the error terms is corrected for. It is a matter of importance in practice how to determine $L$.

$$
\Omega =
\begin{bmatrix}
\sigma_1^2 & \underbrace{\neq 0 \;\; \cdots \;\; \neq 0}_{\text{lag $L$}} & 0 & \cdots & 0 \\
\neq 0     & \sigma_2^2 & \ddots & \ddots & 0  \\
\vdots     & \ddots & \ddots \\
\neq 0          &        &  \\
0          &        &  \\
\vdots          &    &  & \ddots   &  \\
0          &     & &   & \sigma_n^2
\end{bmatrix}, \; (\text{banded matrix form})
$$


### 2. Generlaized Least Square (GLS) Estimation

Approaches using robust standard errors are convenient because they allow for valid inference without requiring explicit modeling or estimation of the form of heteroskedasticity. However, these methods do not guarantee efficiency; as a result, the estimators may be less precise compared to methods that correctly specify and account for the heteroskedasticity structure.

In contrast, GLS addresses these issues by explicitly modeling the form of heteroskedasticity (or autocorrelation), allowing for more efficient estimation when the structure is *correctly* specified. As a result, GLS estimators can achieve greater precision compared to robust standard error approaches.

**Example** 

To grasp the basic idea of GLS, let’s begin with a simple example.

There is a model with heteroskedasticity issue:

$$
\begin{align*}
y_i = \beta_0 + \beta_1 x_i + u_i, \quad \mathrm{Var}(u_i | x_i) = \sigma_i^2 
\end{align*}
$$

Transform the given model:

$$
\begin{align*}
& \frac{y_i}{\sigma_i} = \beta_0 + \beta_1 \frac{x_i}{\sigma_i} + \frac{u_i}{\sigma_i}  \\
\Rightarrow \; & y_i^\star = \beta_0^\star + \beta_1 x_i^\star + u_i^\star, \quad \mathrm{Var}(u_i^\star | x_i) = 1
\end{align*}
$$

In the transformed model, we can run OLS and the estimators are BLUE.
(Note, we are assuming throughout this entire post that A3. condition is met.)

This particular case of GLS is also called Weighted Least Square (WLS) estimation with weights $w_i = \frac{1}{\sigma_i}$. Less weights are assigned to data with greater variance and larger weights are to data with smaller variance.  


**Generalized**

More generally, GLS can be thought of applying transformation to the model and running OLS on the transformed model to obtain BLUE. 

$$
\mathbf{y} = X \boldsymbol{\beta} + \mathbf{u}, \quad \mathrm{Var}(\mathbf{u} | X) = \Omega \neq \sigma^2 I
$$


GLS estimation:

$$
\hat{\boldsymbol{\beta}}_{GLS} = \arg \min_\boldsymbol{\beta} Q(\boldsymbol{\beta}) (= \mathbf{u}^{\top} \Omega^{-1} \mathbf{u}) = \arg \min_\boldsymbol{\beta} (\mathbf{y} - X\boldsymbol{\beta})^{\top} \Omega^{-1} (\mathbf{y} - X\boldsymbol{\beta}) \quad (1)
$$

In GLS, we use the Mahalanobis distance because the error terms may have different variances (heteroskedasticity) or may be correlated (autocorrelation). If we simply minimized the sum of squared residuals (as in OLS), we would be treating all errors as equally reliable, which is not appropriate when their variances differ or when they are correlated. By using we are effectively standardizing the residuals according to their variance-covariance structure. For more details on Mahalanobis distance, refer to: Angelo Yeo, ["Mahalanobis distance"](https://angeloyeo.github.io/2022/09/28/Mahalanobis_distance_en.html), 2022.


**First Order Condition (FOC):**

First order derivatives of each term above:

$$
\begin{align*}
&\frac{\partial}{\partial \boldsymbol{\beta}} Q(\boldsymbol{\beta}) = 0 \\\\
\Rightarrow \; &\hat{\boldsymbol{\beta}}_{GLS} = (X^\top \Omega^{−1} X)^{−1} X^\top \Omega^{-1} \mathbf{y} 
\end{align*}
$$


**Transformation**

Back to the previous example, the idea of GLS is to transform a given model and running OLS. Let's see how this idea is equivalent to the least-square optimization probelms we saw in $(1)$. 

$\Omega$: positive definite and symmetric matrix $\Rightarrow$ orthogonally diagonalizable.

$$
\Omega = C \Lambda C^\top \Rightarrow \Omega^{-1} = T^\top T
$$

- Note that $T$ is not unique; $T = A \Lambda^{-1/2} C^\top$ and A can be any orthogonal matrix.

Transform both sides by multiplying $T$;

$$
T \mathbf{y} = T X \boldsymbol{\beta} + T  \mathbf{u}, \quad \mathrm{Var}(\mathbf{u} | X) = \Omega \neq \sigma^2 I
$$

See how the objective function can be expressed with $T$:

$$
 Q(\boldsymbol{\beta}) = \mathbf{u}^{\top} \Omega^{-1} \mathbf{u} = \| T (\mathbf{y} - X \boldsymbol{\beta}) \|^2 = \|\mathbf{y}^\star - X^\star \boldsymbol{\beta} \|^2
$$

Therefore

$$
\hat{\boldsymbol{\beta}}_{GLS} = \arg \min_\boldsymbol{\beta} (\mathbf{y} - X\boldsymbol{\beta})^{\top} \Omega^{-1} (\mathbf{y} - X\boldsymbol{\beta}) \iff \arg \min_\boldsymbol{\beta} \|\mathbf{y}^\star - X^\star \boldsymbol{\beta} \|^2 \quad (2)
$$

Because $(1) \iff (2)$, $\hat{\boldsymbol{\beta}}_{GLS}$ is OLS estimator of the transformed model.

**Homoskedasticity of Transformed Model**

Transform both hand sides by multiplying $T$;

$$
T \mathbf{y} = T X \boldsymbol{\beta} + T  \mathbf{u}
$$


$$
\mathbf{y}^\star = X^\star \boldsymbol{\beta} + \mathbf{u}^\star
$$

$$
\mathrm{Var}(\mathbf{u}^\star \mid X) = \mathrm{Var}(T\mathbf{u} \mid X) = T\,\Omega\,T^\top = I \quad (3)
$$

Therefore, in the transformed model, the error terms are homoskedastic and uncorrelated.

**Conclusion**

GLS estimator is OLS estimator of the transformed model and the transformed model is transformed in a way that homoskedasticity is restored. Therefore GLS estimator is unbiased and BLUE. i.e., not only valid inference is possible but efficiency is guaranteed. 

#### 2.1 Feasible GLS

True $\Omega$ is unknwon; so $\widehat{\Omega}$ is used instead (estimated variance-covariance matrix).  

### Ending Remark


Robust standard errors (such as White or Newey-West) are convenient because they provide valid inference without requiring explicit modeling of the error variance or correlation structure. However, they do not guarantee efficiency, and the resulting estimators may be less precise. In contrast, GLS explicitly models the error structure and, when specified correctly, yields more efficient and precise estimators. The drawback is that GLS requires correct knowledge or estimation of the error variance-covariance matrix; if this is misspecified, inference may be invalid.

*Informal Summary:*

Robust Standard Error - I don't care about heteroskedasticity, just run OLS and inference procedures; yet the variance might be too large for liking

GLS - Transforrm model to adjust for existing heteroskedasticity, and make estimators BLUE again; yet modelling the vary heteroskedasticity might turn out to be challenging. 

---

**Credit**\
All contents in this post are from a digitized version of my own lecture notes taken during *Econometric Theory I* (Fall 2019, Sungkyunkwan University, [**Prof. Heejoon Han** (Personal Homepage Link)](https://sites.google.com/site/heejoonecon/)) and my own self-study materials & other sources. For specific references worth noting, I included in each relevant part.

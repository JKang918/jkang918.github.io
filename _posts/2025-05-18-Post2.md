---
title: Econometrics > 1. Ordinary Least Square Model
date: 2025-05-18 #HH:MM:SS +/-TTTT
categories: [Econometrics]
tags: [econometrics, ols]     # TAG names should always be lowercase
author: <author_id>                     # for single entry
description: Theoratical Econometrics
toc: true
math: true
mermaid: true
comments: true
---

We begin with linear models, which are among the most widely used and powerful tools in econometrics. First, let’s take a closer look at the Ordinary Least Squares (OLS) model—a regression model based on the least squares method. For students with a background in statistics, the term “multiple linear regression” may be more familiar.

In **1. Ordinary Least Square Model**, we will explore the assumptions underlying OLS, review the mathematical details of each assumption, and discuss alternative approaches that can be considered when these assumptions are not met.

A1. Linearity and A2. No Perfect Collinearity are covered in this post.

## Linear Model: Assumptions

- **A1. Linearity**  
  → Fundamental assumption

- **A2. No Perfect Collinearity**  
  → Indispensable (if violated: estimation is impossible)

- **A3. Zero Conditional Mean**  
  → If violated: use IV (instrumental variables) estimation  
  → **(A1–A3) Required for unbiased estimation**

- **A4. Homoskedasticity & No Autocorrelation**  
  → If violated:  
    - Use robust standard errors  
    - Use GLS or Cochrane-Orcutt  
  → **If (A1–A4) are met:**  
    
$$
\hat{\beta}_{\text{OLS}} \text{ is BLUE (Best Linear Unbiased Estimator)}
$$

$$
\text{Unbiased } \hat{\sigma}^2
$$

$$
\mathrm{Var}(\hat{\beta} \mid X) = \sigma^2 (X^\top X)^{-1}
$$

- **A5. Normality**  
  → If violated: rely on asymptotic properties  
  → **(A1–A5) Required for t-test and F-test**

---

### A1. Linearity

This is the most fundamental assumption. Basically, the true relationship you attempt to substantiate with empirical data and econometric method must have underlying linear relationship between the independant variable and the explanatory variables.

$$
y = X\beta + u
$$

where:

$$
y = \begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}_{n \times 1}, \quad
X = \begin{bmatrix}
1 & x_{11} & x_{12} & \cdots & x_{1k} \\
1 & x_{21} & x_{22} & \cdots & x_{2k} \\
\vdots & \vdots & \vdots & & \vdots \\
1 & x_{n1} & x_{n2} & \cdots & x_{nk}
\end{bmatrix}_{n \times (k+1)}
$$

$$
\beta = \begin{bmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_k
\end{bmatrix}_{(k+1) \times 1}, \quad
u = \begin{bmatrix}
u_1 \\
u_2 \\
\vdots \\
u_n
\end{bmatrix}_{n \times 1}
$$

---

### A2. No Perfect Collinearity

- $X$ has **full column rank** (i.e., columns are linearly independent).


The OLS estimator $\hat{\beta}$ minimizes the sum of squared residuals:

$$
\hat{\beta} = \arg \min_\beta \sum_{i=1}^n u_i^2 = \arg \min_\beta \mathbf{u}^\top \mathbf{u} = \arg \min_\beta (y - X\beta)^\top (y - X\beta)
$$

Expand the expression:


$$
\begin{aligned}
&(y - X\beta)^\top (y - X\beta) \\\\
&= y^\top y - \beta^\top X^\top y - y^\top X \beta + \beta^\top X^\top X \beta \\\\
&= y^\top y - 2\beta^\top X^\top y + \beta^\top X^\top X \beta
\end{aligned}
$$


**First Order Condition (FOC):**

First order derivatives of each term above:

$$
\begin{aligned}
&\frac{\partial}{\partial \beta} (y^\top y) = 0 \\\\
&\frac{\partial}{\partial \beta} (\beta^\top X^\top y) = X^\top y \\\\
&\frac{\partial}{\partial \beta} (\beta^\top X^\top X \beta) = 2 X^\top X \beta
\end{aligned}
$$

(If you have find it difficult to understand, refer to Technical Note 1.)

Combine:

$$
-2 X^\top y + 2 X^\top X \hat{\beta} = 0
$$

$$
X^\top X \hat{\beta} = X^\top y
$$


If $X^\top X$ is invertible (i.e., $X$ has full column rank):

$$
\hat{\beta}_{\text{OLS}} = (X^\top X)^{-1} X^\top y
$$


**Summary:**  
- $X$ has full column rank $\iff$ $X^\top X$ is invertible $\iff$ **No perfect collinearity**.
- This is why no perfect collinearity condition is indispensable condition in OLS.

Refer to Technical Note 2. if you wonder why "$X$ has full column rank $\iff$ $X^\top X$ is invertible" is the case.

---

#### Technical Note 1. Partial Derivatives of Vectors and Matrices

Skip this part if you are already well aware of vector and/or matrix differentiations.

##### Case 1. $$\frac{\partial \left( \mathbf{a}^T \mathbf{b} \right)}{\partial \mathbf{a}} = \mathbf{b}$$


Let $$\mathbf{a}$$ and $$\mathbf{b}$$ be as follow:


$$
\begin{aligned}
&\mathbf{a}^T = [a_1 \;\; a_2 \;\; \ldots \;\; a_n] \\\\
&\mathbf{b} = \begin{bmatrix}
b_1 \\
b_2 \\
\vdots \\
b_n
\end{bmatrix}
\end{aligned}
$$

Then,
$$
\mathbf{a}^T \mathbf{b} = a_1 b_1 + a_2 b_2 + \cdots + a_n b_n = k
$$

Taking the derivative of $k$ with respect to $\mathbf{a}$:

$$
\frac{\partial k}{\partial \mathbf{a}} =
\begin{bmatrix}
\frac{\partial k}{\partial a_1} \\
\frac{\partial k}{\partial a_2} \\
\vdots \\
\frac{\partial k}{\partial a_n}
\end{bmatrix}
=
\begin{bmatrix}
b_1 \\
b_2 \\
\vdots \\
b_n
\end{bmatrix}
= \mathbf{b}
$$

This is:

$$\frac{\partial \left( \mathbf{a}^T \mathbf{b} \right)}{\partial \mathbf{a}} = \mathbf{b}$$

---

##### Case 2. Partial Derivative of a Quadratic Form

Consider the quadratic form (where $A$ is a symmetric matrix):

$$
\mathbf{b}^T A \mathbf{b} = \sum_{i=1}^n \sum_{j=1}^n b_i a_{ij} b_j = k
$$

We want to compute the gradient with respect to $\mathbf{b}$:

$$
\frac{\partial (\mathbf{b}^T A \mathbf{b})}{\partial \mathbf{b}} = \frac{\partial k}{\partial \mathbf{b}}
=
\begin{bmatrix}
\frac{\partial k}{\partial b_1} \\
\frac{\partial k}{\partial b_2} \\
\vdots \\
\frac{\partial k}{\partial b_n}
\end{bmatrix}
$$

First, expand $k$ as follows:

$$
k = \sum_{i=1}^n \sum_{j=1}^n b_i a_{ij} b_j = \sum_{i=1}^n b_i \left( \sum_{j=1}^n a_{ij} b_j \right)
$$

Now, the partial derivative with respect to $b_\ell$ ($\ell = 1, 2, \ldots, n$):


$$
\begin{aligned}
\frac{\partial k}{\partial b_\ell}
&= \sum_{j=1}^n a_{\ell j} b_j \quad \text{($i = \ell$: the $\ell$-th row of $A$)}
+ \sum_{i=1}^n b_i a_{i \ell} \quad \text{($j = \ell$: the $\ell$-th column of $A$)} \\
&= (A_{\ell \cdot} \mathbf{b}) + (A_{\cdot \ell}^T \mathbf{b})\\
&= 2(A_{\ell \cdot} \mathbf{b}) \qquad \text{(symmetric matrix)}
\end{aligned}
$$

Here,
- $\sum_{j=1}^n a_{\ell j} b_j$ is the dot product of the $\ell$-th **row** of $A$ with $\mathbf{b}$, i.e., $A_{\ell \cdot} \mathbf{b}$,
- $\sum_{i=1}^n b_i a_{i \ell}$ is the dot product of the $\ell$-th **column** of $A$ with $\mathbf{b}$, i.e., $A_{\cdot \ell}^T \mathbf{b}$.


**Notation clarification:**
- $A_{\ell \cdot}$ denotes the $\ell$-th **row** of $A$.
- $A_{\cdot \ell}$ denotes the $\ell$-th **column** of $A$.


> In summary, for each $\ell = 1, 2, \ldots, n$,  
> the partial derivative collects both the $\ell$-th row and $\ell$-th column contributions, which coincide when $A$ is symmetric.

---

Then we can proceed to the partial derivative of $k = \mathbf{b}^T A \mathbf{b}$ with respect to $\mathbf{b}$:


$$
\frac{\partial k}{\partial \mathbf{b}} =
\begin{bmatrix}
2 \mathbf{a}_1^T \mathbf{b} \\
2 \mathbf{a}_2^T \mathbf{b} \\
\vdots \\
2 \mathbf{a}_n^T \mathbf{b}
\end{bmatrix}
=
2
\begin{bmatrix}
\mathbf{a}_1^T \\
\mathbf{a}_2^T \\
\vdots \\
\mathbf{a}_n^T
\end{bmatrix}
\mathbf{b}
=
2A\mathbf{b}
$$

where $\mathbf{a}_k$ denotes the $k$-th column of $A$.

Thus,

$$
\frac{\partial (\mathbf{b}^T A \mathbf{b})}{\partial \mathbf{b}} = 2A\mathbf{b}
$$

---


Now, apply this result to the OLS context, where $A = X^T X$ and $\mathbf{b} = \hat{\boldsymbol{\beta}}$:

$$
\frac{\partial \left( \hat{\boldsymbol{\beta}}^T (X^T X) \hat{\boldsymbol{\beta}} \right)}{\partial \hat{\boldsymbol{\beta}}}
= 2 X^T X \hat{\boldsymbol{\beta}}
$$

---

#### Technical Note 2. Proof: $X^T X$ is invertible if and only if $X$ has full column rank

Let $X \in \mathbb{R}^{n \times (k+1)}$, so $X^T X \in \mathbb{R}^{(k+1) \times (k+1)}$ is symmetric.


**($\Leftarrow$) If $X$ has full column rank, then $X^T X$ is invertible.**

- If $X$ has full column rank, the columns of $X$ span $\mathbb{R}^{k+1}$.
- Unless $\|X^T X\|= 0$, $X^T X$ also spans $\mathbb{R}^{k+1}$ because the columns of $X$ are linearly independent.

Suppose there exists a **non-trivial solution** $\mathbf{b} \neq \mathbf{0}$ such that
$$
X^T X \mathbf{b} = \mathbf{0}.
$$
Then, this implies
$$
|X^T X| = 0
$$
which contradicts the invertibility of $X^T X$.

In more detail:

Suppose such a non-trivial solution exists.

$$
\mathbf{b}^T X^T X \mathbf{b} = 0
$$

But,

$$
(X \mathbf{b})^T (X \mathbf{b}) = \| X \mathbf{b} \|^2 = 0
$$

So,

$$
X \mathbf{b} = \mathbf{0} \qquad \text{with} \quad \mathbf{b} \neq \mathbf{0}
$$

But since $X$ has full column rank, $X \mathbf{b} = \mathbf{0}$ only if $\mathbf{b} = \mathbf{0}$.  
Thus, such a non-trivial $\mathbf{b}$ does not exist.

---

**($\Rightarrow$) If $X^T X$ is invertible, then $X$ has full column rank.**

Suppose $X$ does **not** have full column rank,  
then there exists a **non-trivial solution** $\mathbf{b} \neq 0$ such that
$$
X\mathbf{b} = 0.
$$

Then,
$$
X^T X \mathbf{b} = 0
$$
with $\mathbf{b} \neq 0$ (non-trivial solution).

But if $X^T X$ is invertible, the only solution to $X^T X \mathbf{b} = 0$ is $\mathbf{b}=0$,
which is a contradiction.

**Conclusion:**  
$X^T X$ is invertible $\iff$ $X$ has full column rank, where $X \in \mathbb{R}^{n \times (k+1)}$ and $X^T X \in \mathbb{R}^{(k+1) \times (k+1)}$ is symmetric.

---

#### Technical Note 3. Alternative Approach - Linear Algebra (geometric interpretation)

The OLS estimator is given by:

$$
\hat{\boldsymbol{\beta}}_{OLS} = (X^T X)^{-1} X^T y
$$

This can be understood geometrically.

$$
\hat{\boldsymbol{\beta}}_{\text{OLS}} = \arg\min_{\boldsymbol{\beta}} \| y - \hat{y} \| 
= \arg\min_{\boldsymbol{\beta}} \| y - X \boldsymbol{\beta} \|
$$

where $\hat{y} = \text{Proj}_{\text{col} X} y$


Here, you can see that OLS attempts to explain $y$ as much as possible using the columns of $X$ (i.e., the regression parameters). The resulting $\hat{y}$ represents the best approximation of $y$ within the column space of $X$. Put differently, $\hat{y}$ is the projection of $y$ onto the column space of $X$.

In other words, because $\hat{y}$ is the projection of $y$ onto the column space of $X$, the difference $(y - \hat{y})$ belongs to the orthogonal space (null space) relative to the columns of $X$.

That is:

$$
(y - \hat{y}) \cdot \mathbf{x}_j = 0 \qquad (j = 1, \ldots, k+1)
$$

$$
\iff X^T (y - \hat{y}) = 0
$$

$$
\implies X^T (y - X \hat{\boldsymbol{\beta}}) = 0
$$

Thus,

$$
X^T X \hat{\boldsymbol{\beta}} = X^T y
$$

$$
\hat{\boldsymbol{\beta}}_{\text{OLS}} = (X^T X)^{-1} X^T y
$$

---

A3, A4, A5  will be discussed in future posts.

---

**Credit**\
All content, *except for technical notes*, in this post is a digitized version of my own lecture notes taken during *Econometric Theory I* (Fall 2019, Sungkyunkwan University, [**Prof. Heejoon Han** (Personal Homepage Link)](https://sites.google.com/site/heejoonecon/)).
Full credit for the course materials and original explanations belongs to the professor.
Technical notes and explanations are entirely my original work.

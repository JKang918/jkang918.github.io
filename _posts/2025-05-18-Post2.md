---
title: Stochastic Process > 1. Basic Probability Theory - Conditioning
date: 2025-05-18 #HH:MM:SS +/-TTTT
categories: [Stochastic Process, General (ISYE6650)]
tags: [stochastic process, georgia tech, conditional expectation, conditioning]     # TAG names should always be lowercase
author: <author_id>                     # for single entry
description: Georgia Tech ISYE6650 Fall 2024 LC notes
toc: true
math: true
mermaid: true
---

**Credit**\
All content in this post is a digitized version of my own lecture notes taken during *ISYE 6650: Probabilistic Models and Their Applications* (Fall 2024, Georgia Tech, [**Prof. Sigrun Andradottir** (Georgia Tech Link)](https://www.isye.gatech.edu/users/sigrun-andradottir)).
Full credit for the course materials and original explanations belongs to the professor.

## Basic Laws

$$
\forall X, \: Y \: \text{ : Random Variables, } A \text{: event}
$$


$$
\begin{align*}
\textbf{1.} \quad & \mathbb{E}[X] = \mathbb{E}\big[\mathbb{E}[X|Y]\big] \\[1.2em]
\textbf{2.} \quad & \mathbb{P}(X \in A) = \mathbb{E}\big[\mathbb{P}(X \in A \mid Y)\big] \\[1.2em]
\textbf{3.} \quad & \operatorname{Var}[X] = \operatorname{Var}\big(\mathbb{E}[X|Y]\big) + \mathbb{E}\big[\operatorname{Var}(X|Y)\big]
\end{align*}
$$

(3. is also known as the "Variance Decomposition Formula.")

First, we derive formulas 1–3.

Then, we look into examples showing why 1–3 are useful.

Basically, 1–3 decompose one computation task into two computation tasks.  
Formulas 1–3 are used only when the decomposition is useful (i.e., running two computations is easier than one computation).


## 1. Law of Total Expectation




### (discrete case)

Let $f(x, y)$: joint pmf,  
$f_X(x)$, $f_Y(y)$: marginal pmfs,  
$f_{X|Y}(x|y)$, $f_{Y|X}(y|x)$: conditional pmfs

$$
\begin{align*}
\mathbb{E}[X]
  &= \sum_x x\, f_X(x) \\
  &= \sum_x x \left( \sum_y f(x, y) \right) \\
  &= \sum_x x \left( \sum_y f_Y(y) \frac{f(x, y)}{f_Y(y)} \right) \\
  &= \sum_x x \left( \sum_y f_{X|Y}(x|y) f_Y(y) \right) \\
  &= \sum_y \sum_x x\, f_{X|Y}(x|y) f_Y(y) \\
  &= \sum_y \left( \sum_x x\, f_{X|Y}(x|y) \right) f_Y(y) \\
  &= \sum_y \mathbb{E}[X|Y=y]\, f_Y(y) \\
  &= \mathbb{E}_Y \left[\, \mathbb{E}[X|Y] \,\right]
  & \square
\end{align*}
$$

### (continuous case)

Let $f(x, y)$: joint pdf,  
$f_X(x)$, $f_Y(y)$: marginal pdfs,  
$f_{X|Y}(x|y)$, $f_{Y|X}(y|x)$: conditional pdfs

$$
\begin{align*}
\mathbb{E}[X] 
  &= \int_{-\infty}^{\infty} x\, f_X(x)\, dx \\
  &= \int_{-\infty}^{\infty} x \left( \int_{-\infty}^{\infty} f(x, y)\, dy \right) dx \\
  &= \int_{-\infty}^{\infty} x \left( \int_{-\infty}^{\infty} \frac{f(x, y)}{f_Y(y)} f_Y(y)\, dy \right) dx \\
  &= \int_{-\infty}^{\infty} x \left( \int_{-\infty}^{\infty} f_{X|Y}(x|y) f_Y(y)\, dy \right) dx \\
  &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x f_{X|Y}(x|y) f_Y(y)\, dy\, dx \\
  &= \int_{-\infty}^{\infty} \left( \int_{-\infty}^{\infty} x f_{X|Y}(x|y)\, dx \right) f_Y(y)\, dy \\
  &= \mathbb{E}_Y \left[\, \mathbb{E}[X|Y] \,\right]
  & \square
\end{align*}
$$


---

## 2. Law of Total Probability

### Indicator fuction

$$
\mathbb{P}(X \in A) = 0 \cdot \mathbb{P}(X \notin A) + 1 \cdot \mathbb{P}(X \in A)
$$

Let $I_E$: indicator of event $E$  
(e.g., $I(E)$, $I_E$, $I_{\{E\}}$, $\mathbb{I}_{E}$, etc.)

$$
I_E = 
\begin{cases}
1 & \text{if $E$ occurs} \\
0 & \text{otherwise}
\end{cases}
$$

$$
\mathbb{E}[I_E] = 0 \cdot \mathbb{P}(E^c) + 1 \cdot \mathbb{P}(E) = \mathbb{P}(E)
$$

$$
\mathbb{E}[I_{\{X \in A\}}] = 0 \cdot \mathbb{P}(X \notin A) + 1 \cdot \mathbb{P}(X \in A) = \mathbb{P}(X \in A)
$$


### Back to our proof..


$$
\begin{align*}
\mathbb{P}(X \in A)
    &= \mathbb{E}\big[ I_{\{X \in A\}} \big] \\
    &= \mathbb{E}\Big[\, \mathbb{E}\big[ I_{\{X \in A\}} \mid Y \big]\,\Big] \qquad \text{(by the law of total expectation)} \\
    &= \mathbb{E}\Big[\, \mathbb{P}(X \in A \mid Y)\,\Big]
    & \square
\end{align*}
$$


---

## 3. Variance Decomposition Formula

$$
\begin{align*}
\operatorname{Var}[X]
    &= \mathbb{E}[X^2] - (\mathbb{E}[X])^2 \\
    &= \mathbb{E}\big[\, \mathbb{E}[X^2 \mid Y]\, \big] - \left( \mathbb{E}\big[\, \mathbb{E}[X \mid Y]\, \big] \right)^2 \qquad \text{(by the law of total expectation)} \\
    &= \mathbb{E}\big[\, \mathbb{E}[X^2 \mid Y]\, \big] - \mathbb{E}\big[\, \mathbb{E}[X \mid Y]\, \big]^2 \\
    &\quad + \mathbb{E}\big[\, \mathbb{E}[X \mid Y]^2\, \big] - \mathbb{E}\big[\, \mathbb{E}[X \mid Y]\, \big]^2 \\
    &= \mathbb{E}\big[\, \mathbb{E}[X^2 \mid Y] - \mathbb{E}[X \mid Y]^2\, \big]
    + \operatorname{Var}\big( \mathbb{E}[X \mid Y] \big) \\
    &= \mathbb{E}\big[\, \operatorname{Var}(X \mid Y) \, \big] + \operatorname{Var}\big( \mathbb{E}[X \mid Y] \big)
    & \square
    \end{align*}
$$

---

## Example 1. Random Route Travel Time

Now, let's take a look at the example showing how powerful conditioning techniques can be. 

Suppose you travel from $A$ to $B$ by one of two routes:

![Route Selection Diagram](/assets/img/route_diagram.svg)

- **Route 1:** probability $0.7$, $T_1 \sim U[9, 14]$
- **Route 2:** probability $0.3$, $T_2 \sim U[10, 16]$

Let $T$ be the travel time.

### Little reminder: uniform distribution properties

For $U[a, b]$:

$$
\text{Mean:} \quad \frac{a+b}{2}
$$

$$
\text{Variance:} \quad \frac{(b-a)^2}{12}
$$

---

### Calculations

#### $\mathbb{E}[T]$

$$
\mathbb{E}[T] = 0.7 \times \mathbb{E}[T|R_1] + 0.3 \times \mathbb{E}[T|R_2]
$$

$$
\mathbb{E}[T|R_1] = \frac{9+14}{2} = 11.5
$$

$$
\mathbb{E}[T|R_2] = \frac{10+16}{2} = 13
$$

$$
\mathbb{E}[T] = 0.7 \times 11.5 + 0.3 \times 13 = 11.95
$$

---

#### $\mathbb{P}(T < 11)$

$$
\begin{align*}
\mathbb{P}(T < 11)
    &= \mathbb{E}\left[\, \mathbb{P}(T < 11 \mid R) \, \right] \\
    &= 0.7 \cdot \mathbb{P}(T < 11 \mid R = R_1) + 0.3 \cdot \mathbb{P}(T < 11 \mid R = R_2) \\
    &= 0.7 \cdot \frac{1}{3} + 0.3 \cdot \frac{1}{5} \\
    &= 0.7 \times 0.333\ldots + 0.3 \times 0.2 \\
    &= 0.233\ldots + 0.06 \\
    &= 0.293\ldots
\end{align*}
$$

---

#### $\operatorname{Var}(T)$

$$
\operatorname{Var}(T) = \mathbb{E}[\operatorname{Var}(T|R)] + \operatorname{Var}(\mathbb{E}[T|R])
$$

$$
\operatorname{Var}(T|R_1) = \frac{(14-9)^2}{12} = \frac{25}{12}
$$

$$
\operatorname{Var}(T|R_2) = \frac{(16-10)^2}{12} = \frac{36}{12} = 3
$$

$$
\mathbb{E}[\operatorname{Var}(T|R)] = 0.7 \times \frac{25}{12} + 0.3 \times 3
$$

$$
\operatorname{Var}(\mathbb{E}[T|R]) = 0.7 \times (11.5 - 11.95)^2 + 0.3 \times (13 - 11.95)^2
$$

---

#### Hidden Assumption

- Independence assumption: travel time information is not used for route decision
- Meaning, if one knows the information beforehand, he would have chosen the route 1 for sure all the time!

---

## Example 2. Earlang Distribution


$X, Y$: independent exponential RVs with common rate $\lambda$

$$(X, Y) \overset{\text{i.i.d.}}{\sim} \mathrm{Exp}(\lambda)$$

Let $Z = X + Y$. Get the cdf of $Z$.

---

$$
F_Z(z) = P(X + Y \leq z) =
\begin{cases}
0 & (z < 0) \\\\
? & (z \geq 0)
\end{cases}
$$


**Step-by-step:** ($z \geq 0$)

$$
\begin{aligned}
F_Z(z) &= P(Z \leq z) = P(X + Y \leq z) \\\\
&= \mathbb{E}\left[P(X + Y \leq z \mid Y)\right] \quad \text{(conditioning)} \\\\
&= \int_{-\infty}^{\infty} P(X + Y \leq z \mid Y = y) f_Y(y)\, dy \\\\
&= \int_{0}^{\infty} P(X \leq z - y) f_Y(y)\, dy \qquad (f_Y(y) = 0 \text{ for } y < 0) \\\\
&= \int_{0}^{z} P(X \leq z - y)\, \lambda e^{-\lambda y} \, dy \\\\
&= \int_{0}^{z} F_X(z-y)\, \lambda e^{-\lambda y} \, dy
\end{aligned}
$$

---

$F_X(x) = 1 - e^{-\lambda x}$ for $x \geq 0$  
Thus,

$$
F_X(z-y) =
\begin{cases}
1 - e^{-\lambda (z-y)} & (z \geq y) \\\\
0 & (z < y)
\end{cases}
$$

---

Plug in $F_X(z-y)$:

$$
\begin{aligned}
F_Z(z) &= \int_0^z [1 - e^{-\lambda(z-y)}] \lambda e^{-\lambda y}\, dy \\\\
&= \int_0^z \lambda e^{-\lambda y}\, dy - \int_0^z e^{-\lambda(z-y)} \lambda e^{-\lambda y}\, dy \\\\
&= \int_0^z \lambda e^{-\lambda y}\, dy - \lambda e^{-\lambda z} \int_0^z 1\, dy \\\\
&= [1 - e^{-\lambda z}] - \lambda e^{-\lambda z} \cdot z
\end{aligned}
$$

---

Therefore,

$$
F_Z(z) =
\begin{cases}
0 & (z < 0) \\\\
1 - e^{-\lambda z} - z\lambda e^{-\lambda z} & (z \geq 0)
\end{cases}
$$

---
Here, $Z$ is called, "Erlang" random variable. Also, Erlang distribution is a special case of Gamma distribution.

$Z \sim \mathrm{Gamma}(2, \lambda)$  
$Z \sim \mathrm{Erlang}(2, \lambda)$

---

**Key takeaways:**

1. Conditioning is useful sometimes.
2. $X_1, ..., X_n \overset{\text{i.i.d.}}{\sim} \mathrm{Exp}(\lambda) \implies X_1 + ... + X_n \sim \mathrm{Erlang}(n, \lambda)$

We will discuss the pdf of the Erlang distribution in detail in a future post.

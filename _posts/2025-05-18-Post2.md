---
title: Econometrics > 1. Ordinary Least Square Model: Fundamental Assumptions
date: 2025-05-18 #HH:MM:SS +/-TTTT
categories: [Econometrics]
tags: [econometrics, ols]     # TAG names should always be lowercase
author: <author_id>                     # for single entry
description: Theoretical Econometrics
toc: true
math: true
mermaid: true
comments: true
---

We begin with linear models, which are among the most widely used and powerful tools in econometrics. First, let’s take a closer look at the Ordinary Least Squares (OLS) model—a regression model based on the least squares method. For students with a background in statistics, the term “multiple linear regression” may be more familiar.

In **1. Ordinary Least Squares (OLS)**, we begin by rigorously examining the underlying assumptions of the model. You may have encountered these assumptions in undergraduate statistics courses; however, here we will analyze them more thoroughly using matrix equations. This approach will help clarify exactly how each assumption is related to different elements of the model, as well as the consequences when certain assumptions are violated.

Building on a solid understanding of OLS, we will then explore methods for measuring a model’s explanatory power, the effects of failing to achieve the optimal model specification (such as omitted variables or redundant variables), multicollinearity, two-step regression, partialling out, and forecasting methods using OLS—covering each topic in detail one by one.

> This post is quite long! In fact, it covers material equivalent to about two months of a typical university lecture, so there’s no need to feel discouraged if you don’t understand everything in a single day. If you take your time and reflect on each concept, you will be able to understand it fully. Of course, if you manage to grasp everything at once, congratulations—you are truly exceptional!

## Linear Model: Assumptions

Let’s take a top-down approach and first review what OLS assumptions there are. 

- **A1. Linearity**  

This is the most fundamental aspect of OLS. In fact, it is not only an assumption for OLS, but also a basic premise of general linear models. There must be a true linear relationship between the explanatory variables and the dependent variable. If this condition is not satisfied, it would not make sense to use any linear model at all.  

- **A2. No Perfect Collinearity**  

If this assumption is not satisfied, OLS estimation becomes impossible. When Assumption 2 is met, the OLS estimator can be derived in the well-known closed form shown below:


$$
\hat{\beta}_{\text{OLS}} = (X^{\top} X)^{-1} X^{\top} y
$$


- **A3. Zero Conditional Mean**  

$$
\mathbb{E}[u \mid X] = 0
$$

This assumption means that the expected value of the residual vector, as an estimator of the error vector, is the zero vector. When A1 through A3 are satisfied, the OLS estimator achieves unbiasedness.


$$
\mathbb{E}\left[\,\hat{\beta} \mid X\,\right] = \beta \qquad \text{(Under A1, A2, A3)}
$$


When A3 is violated, the OLS estimator becomes biased. In such cases, the instrumental variable (IV) estimation method can be used. I will discuss this method in a separate post.

- **A4. Homoskedasticity & No Autocorrelation**  

A4 assumes that the errors are “equally variable” and “independent” across all observations.

$$
\mathrm{Var}[\mathbf{u} \mid X] = \mathbb{E}[\mathbf{u} \mathbf{u}^{\top} \mid X]  = \sigma^2 I
$$

When A1 through A4 are met, the OLS estimator is BLUE - Best Linear Unbiased Estimator and the error variance estimator $\hat{\sigma}^2$ is also an unbiased estimator. And the covariance of matrix $\hat{\beta}$ can be expressed in a closed form.

$$
\hat{\beta}_{\text{OLS}} \text{ is BLUE (Best Linear Unbiased Estimator)}
$$

$$
\text{Unbiased } \hat{\sigma}^2
$$

$$
\mathrm{Var}(\hat{\beta} \mid X) = \sigma^2 (X^{\top} X)^{-1}
$$


When A4 is violated we can use either *robust standard errors* or *GLS* or *Cochrane-Orcutt*. We will take a look at these methods later.


- **A5. Normality**  

$$
u \mid X \sim \mathcal{N}(0,\, \sigma^2 I)
$$

This means that the error vector $u$, conditional on $X$, follows a multivariate normal distribution with mean zero and covariance matrix $\sigma^2 I$. In other words, the errors are independently and identically distributed (i.i.d.), each with mean zero and variance $\sigma^2$.

Assumptions A1 to A5 are required for performing t-test and F-test. However, even when A5 is violated, with certain conditions met, we can still run t-test and F-test relying on *asymptotic properties*. We will learn what these are later.

---

### A1. Linearity

This is the most fundamental assumption. Basically, the true relationship you attempt to substantiate with empirical data and econometric method must have underlying linear relationship between the independent variable and the explanatory variables.

There are $n$ data points and $k$ parameters.

$$
y = X\beta + u
$$

where:

$$
y = \begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}_{n \times 1}, \quad
X = \begin{bmatrix}
1 & x_{11} & x_{12} & \cdots & x_{1k} \\
1 & x_{21} & x_{22} & \cdots & x_{2k} \\
\vdots & \vdots & \vdots & & \vdots \\
1 & x_{n1} & x_{n2} & \cdots & x_{nk}
\end{bmatrix}_{n \times (k+1)}
$$

$$
\beta = \begin{bmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_k
\end{bmatrix}_{(k+1) \times 1}, \quad
u = \begin{bmatrix}
u_1 \\
u_2 \\
\vdots \\
u_n
\end{bmatrix}_{n \times 1}
$$

---

### A2. No Perfect Collinearity

The meaning of "No Perfect Collinearity" is:

- $X$ has **full column rank** (i.e., columns are linearly independent).

With A2 satisfied, we can derive the closed form of $\hat{\beta}_{OLS}$.

The OLS estimator $\hat{\beta}$ minimizes the sum of squared residuals:

$$
\hat{\beta} = \arg \min_\beta \sum_{i=1}^n u_i^2 = \arg \min_\beta \mathbf{u}^{\top} \mathbf{u} = \arg \min_\beta (y - X\beta)^{\top} (y - X\beta)
$$

Expand the expression:

$$
\begin{aligned}
&(y - X\beta)^{\top} (y - X\beta) \\\\
&= y^{\top} y - \beta^{\top} X^{\top} y - y^{\top} X \beta + \beta^{\top} X^{\top} X \beta \\\\
&= y^{\top} y - 2\beta^{\top} X^{\top} y + \beta^{\top} X^{\top} X \beta
\end{aligned}
$$

**First Order Condition (FOC):**

First order derivatives of each term above:

$$
\begin{aligned}
&\frac{\partial}{\partial \beta} (y^{\top} y) = 0 \\\\
&\frac{\partial}{\partial \beta} (\beta^{\top} X^{\top} y) = X^{\top} y \\\\
&\frac{\partial}{\partial \beta} (\beta^{\top} X^{\top} X \beta) = 2 X^{\top} X \beta
\end{aligned}
$$

(If you have find it difficult to understand, refer to [Technical Note 1](#technical-note-1-partial-derivatives-of-vectors-and-matrices).)

Combine:

$$
-2 X^{\top} y + 2 X^{\top} X \hat{\beta} = 0
$$

$$
X^{\top} X \hat{\beta} = X^{\top} y
$$


If $X^{\top} X$ is invertible (i.e., $X$ has full column rank):

$$
\hat{\beta}_{\text{OLS}} = (X^{\top} X)^{-1} X^{\top} y
$$

> Why do we not check the second order condition? $\because$ The function is convex. We are minimizing the sum of "square"s.

**Summary:**  
- $X$ has full column rank $\iff$ $X^{\top} X$ is invertible $\iff$ **No perfect collinearity**.
- This is why no perfect collinearity condition is indispensable condition in OLS.

Refer to [Technical Note 2](#technical-note-2-proof--is-invertible-if-and-only-if--has-full-column-rank). if you wonder why "$X$ has full column rank $\iff$ $X^{\top} X$ is invertible" is the case.

---

#### Alternative Approach - Linear Algebra (geometric interpretation)

The OLS estimator is given by:

$$
\hat{\boldsymbol{\beta}}_{OLS} = (X^{\top} X)^{-1} X^{\top} y
$$

This can be understood geometrically.

$$
\hat{\boldsymbol{\beta}}_{\text{OLS}} = \arg\min_{\boldsymbol{\beta}} \| y - \hat{y} \| 
= \arg\min_{\boldsymbol{\beta}} \| y - X \boldsymbol{\beta} \|
$$

where $\hat{y} = \text{Proj}_{\text{col} X} y$


Here, you can see that OLS attempts to explain $y$ as much as possible using the columns of $X$ (i.e., the regression parameters). The resulting $\hat{y}$ represents the best approximation of $y$ within the column space of $X$. Put differently, $\hat{y}$ is the projection of $y$ onto the column space of $X$.

In other words, because $\hat{y}$ is the projection of $y$ onto the column space of $X$, the difference $(y - \hat{y})$ belongs to the orthogonal space (null space) relative to the columns of $X$.

That is:

$$
(y - \hat{y}) \cdot \mathbf{x}_j = 0 \qquad (j = 1, \ldots, k+1)
$$

$$
\implies X^{\top} (y - \hat{y}) = 0
$$

$$
\iff X^{\top} (y - X \hat{\boldsymbol{\beta}}) = 0
$$

Thus,

$$
X^{\top} X \hat{\boldsymbol{\beta}} = X^{\top} y
$$

$$
\hat{\boldsymbol{\beta}}_{\text{OLS}} = (X^{\top} X)^{-1} X^{\top} y
$$


This section is very important in understanding the concept of "residual maker" in the later part of this post:

[Residual Maker Section](#projection--residual-maker)

---

#### Technical Note 1. Partial Derivatives of Vectors and Matrices

Skip this part if you are already well aware of vector and/or matrix differentiations.

##### Case 1. $$\frac{\partial \left( \mathbf{a}^{\top} \mathbf{b} \right)}{\partial \mathbf{a}} = \mathbf{b}$$


Let $$\mathbf{a}$$ and $$\mathbf{b}$$ be as follow:


$$
\begin{aligned}
&\mathbf{a}^{\top} = [a_1 \;\; a_2 \;\; \ldots \;\; a_n] \\\\
&\mathbf{b} = \begin{bmatrix}
b_1 \\
b_2 \\
\vdots \\
b_n
\end{bmatrix}
\end{aligned}
$$

Then,
$$
\mathbf{a}^{\top} \mathbf{b} = a_1 b_1 + a_2 b_2 + \cdots + a_n b_n = k
$$

Taking the derivative of $k$ with respect to $\mathbf{a}$:

$$
\frac{\partial k}{\partial \mathbf{a}} =
\begin{bmatrix}
\frac{\partial k}{\partial a_1} \\
\frac{\partial k}{\partial a_2} \\
\vdots \\
\frac{\partial k}{\partial a_n}
\end{bmatrix}
=
\begin{bmatrix}
b_1 \\
b_2 \\
\vdots \\
b_n
\end{bmatrix}
= \mathbf{b}
$$

This is:

$$\frac{\partial \left( \mathbf{a}^{\top} \mathbf{b} \right)}{\partial \mathbf{a}} = \mathbf{b}$$

---

##### Case 2. Partial Derivative of a Quadratic Form

Consider the quadratic form (where $A$ is a symmetric matrix):

$$
\mathbf{b}^{\top} A \mathbf{b} = \sum_{i=1}^n \sum_{j=1}^n b_i a_{ij} b_j = k
$$

We want to compute the gradient with respect to $\mathbf{b}$:

$$
\frac{\partial (\mathbf{b}^{\top} A \mathbf{b})}{\partial \mathbf{b}} = \frac{\partial k}{\partial \mathbf{b}}
=
\begin{bmatrix}
\frac{\partial k}{\partial b_1} \\
\frac{\partial k}{\partial b_2} \\
\vdots \\
\frac{\partial k}{\partial b_n}
\end{bmatrix}
$$

First, expand $k$ as follows:

$$
k = \sum_{i=1}^n \sum_{j=1}^n b_i a_{ij} b_j = \sum_{i=1}^n b_i \left( \sum_{j=1}^n a_{ij} b_j \right)
$$

Now, the partial derivative with respect to $b_\ell$ ($\ell = 1, 2, \ldots, n$):


$$
\begin{aligned}
\frac{\partial k}{\partial b_\ell}
&= \sum_{j=1}^n a_{\ell j} b_j \quad \text{($i = \ell$: the $\ell$-th row of $A$)}
+ \sum_{i=1}^n b_i a_{i \ell} \quad \text{($j = \ell$: the $\ell$-th column of $A$)} \\
&= (A_{\ell \cdot} \mathbf{b}) + (A_{\cdot \ell}^{\top} \mathbf{b})\\
&= 2(A_{\ell \cdot} \mathbf{b}) \qquad \text{(symmetric matrix)}
\end{aligned}
$$

Here,
- $\sum_{j=1}^n a_{\ell j} b_j$ is the dot product of the $\ell$-th **row** of $A$ with $\mathbf{b}$, i.e., $A_{\ell \cdot} \mathbf{b}$,
- $\sum_{i=1}^n b_i a_{i \ell}$ is the dot product of the $\ell$-th **column** of $A$ with $\mathbf{b}$, i.e., $A_{\cdot \ell}^{\top} \mathbf{b}$.


**Notation clarification:**
- $A_{\ell \cdot}$ denotes the $\ell$-th **row** of $A$.
- $A_{\cdot \ell}$ denotes the $\ell$-th **column** of $A$.


> In summary, for each $\ell = 1, 2, \ldots, n$,  
> the partial derivative collects both the $\ell$-th row and $\ell$-th column contributions, which coincide when $A$ is symmetric.


Then we can proceed to the partial derivative of $k = \mathbf{b}^{\top} A \mathbf{b}$ with respect to $\mathbf{b}$:


$$
\frac{\partial k}{\partial \mathbf{b}} =
\begin{bmatrix}
2 \mathbf{a}_1^{\top} \mathbf{b} \\
2 \mathbf{a}_2^{\top} \mathbf{b} \\
\vdots \\
2 \mathbf{a}_n^{\top} \mathbf{b}
\end{bmatrix}
=
2
\begin{bmatrix}
\mathbf{a}_1^{\top} \\
\mathbf{a}_2^{\top} \\
\vdots \\
\mathbf{a}_n^{\top}
\end{bmatrix}
\mathbf{b}
=
2A\mathbf{b}
$$

where $\mathbf{a}_k$ denotes the $k$-th column of $A$.

Thus,

$$
\frac{\partial (\mathbf{b}^{\top} A \mathbf{b})}{\partial \mathbf{b}} = 2A\mathbf{b}
$$


Now, apply this result to the OLS context, where $A = X^{\top} X$ and $\mathbf{b} = \hat{\boldsymbol{\beta}}$:

$$
\frac{\partial \left( \hat{\boldsymbol{\beta}}^{\top} (X^{\top} X) \hat{\boldsymbol{\beta}} \right)}{\partial \hat{\boldsymbol{\beta}}}
= 2 X^{\top} X \hat{\boldsymbol{\beta}}
$$

---

#### Technical Note 2. Proof: $X^{\top} X$ is invertible if and only if $X$ has full column rank

Let $X \in \mathbb{R}^{n \times (k+1)}$, so $X^{\top} X \in \mathbb{R}^{(k+1) \times (k+1)}$ is symmetric.


**($\Leftarrow$) If $X$ has full column rank, then $X^{\top} X$ is invertible.**

- If $X$ has full column rank, the columns of $X$ span $\mathbb{R}^{k+1}$.
- Unless $\|X^{\top} X\|= 0 $, $X^{\top} X$ also spans $\mathbb{R}^{k+1}$ because the columns of $X$ are linearly independent.

Suppose there exists a **non-trivial solution** $\mathbf{b} \neq \mathbf{0}$ such that
$$
X^{\top} X \mathbf{b} = \mathbf{0}.
$$
Then, this implies
$$
\|X^{\top} X\| = 0
$$
which contradicts the invertibility of $X^{\top} X$.

In more detail:

Suppose such a non-trivial solution exists.

$$
\mathbf{b}^{\top} X^{\top} X \mathbf{b} = 0
$$

But,

$$
(X \mathbf{b})^{\top} (X \mathbf{b}) = \| X \mathbf{b} \|^2 = 0
$$

So,

$$
X \mathbf{b} = \mathbf{0} \qquad \text{with} \quad \mathbf{b} \neq \mathbf{0}
$$

But since $X$ has full column rank, $X \mathbf{b} = \mathbf{0}$ only if $\mathbf{b} = \mathbf{0}$.  
Thus, such a non-trivial $\mathbf{b}$ does not exist.

---

**($\Rightarrow$) If $X^{\top} X$ is invertible, then $X$ has full column rank.**

Suppose $X$ does **not** have full column rank,  
then there exists a **non-trivial solution** $\mathbf{b} \neq 0$ such that
$$
X\mathbf{b} = 0.
$$

Then,
$$
X^{\top} X \mathbf{b} = 0
$$
with $\mathbf{b} \neq 0$ (non-trivial solution).

But if $X^{\top} X$ is invertible, the only solution to $X^{\top} X \mathbf{b} = 0$ is $\mathbf{b}=0$,
which is a contradiction.

**Conclusion:**  
$X^{\top} X$ is invertible $\iff$ $X$ has full column rank, where $X \in \mathbb{R}^{n \times (k+1)}$ and $X^{\top} X \in \mathbb{R}^{(k+1) \times (k+1)}$ is symmetric.

---

### A3. Zero Conditional Mean


$$
\mathbb{E}[u \mid X] = 0
$$

Let's try to make sense of the intuitive meaning of A3. Zero Conditional Mean:

- Zero conditional mean $\iff$ exogeneity of explanatory variables



$$
\begin{align*}
\text{Cov}(u_i, x_{mj}) 
    &= \mathbb{E}[u_i x_{mj}] - \mathbb{E}[u_i]\mathbb{E}[x_{mj}] \qquad \forall i, \; m, \; j \\
    &= \mathbb{E}[u_i x_{mj}] \\
    &= \mathbb{E}\big[\, \mathbb{E}[u_i x_{mj} \mid X]\,\big] \quad \text{(law of iterated expectation)} \\
    &= \mathbb{E}\big[\, x_{mj} \mathbb{E}[u_i \mid X]\,\big] \\
    &= \mathbb{E}\big[\, x_{mj} \cdot 0 \,\big] \quad \text{(zero conditional mean)} \\
    &= 0
\end{align*}
$$

> If the error term ($u$) and the explanatory variable ($X$) have zero correlation, it means that there is no systematic relationship between them. If $u$ and $X$ are uncorrelated, the estimator of $\beta$ is unbiased, meaning it is centered around the true value on average.

#### Theorem 1. Unbiased Estimation

With A1 - A3 satisfied, $\hat{\beta}_{\text{OLS}}$ is an unbiased estimator of $\beta$.

$$
\mathbb{E}\left[\,\hat{\beta} \mid X\,\right] = \beta \qquad \text{(Under A1, A2, A3)}
$$

**Proof:**

$$
\begin{align*}
\hat{\beta} &= (X^{\top} X)^{-1} X^{\top} y \quad \text{(A2)} \\
            &= (X^{\top} X)^{-1} X^{\top} (X\beta + u) \quad \text{(A1)} \\
            &= \beta + (X^{\top} X)^{-1} X^{\top} u \\
\end{align*}
$$

With that,

$$
\begin{align*}
\mathbb{E}[\hat{\beta} \mid X]
            &= \mathbb{E}\left[\, \beta + (X^{\top} X)^{-1} X^{\top} u \mid X \,\right] \\
            &= \beta + (X^{\top} X)^{-1} X^{\top} \mathbb{E}[u \mid X] \\
            &= \beta + (X^{\top} X)^{-1} X^{\top} \cdot 0 \quad \text{(A3)} \\
            &= \beta
            &\square
\end{align*}
$$


---

### A4. Homoskedasticity & No Autocorrelation

Assumption A4 is composed of two parts:

1. **Homoskedasticity**: The variance of the error terms is constant for given $X$:
   
   $$
   \mathrm{Var}[u_i \mid X] = \sigma^2, \qquad \forall i
   $$

2. **No Autocorrelation**: The error terms are uncorrelated with each other (for $i \neq j$):
   
   $$
   \mathrm{Cov}(u_i, u_j \mid X) = 0, \qquad \forall i \neq j
   $$

We can write these two assumptions together as:
$$
\mathrm{Var}[\mathbf{u} \mid X] = \sigma^2 I
$$
or equivalently,
$$
\mathbb{E}[\mathbf{u} \mathbf{u}^{\top} \mid X] = \sigma^2 I
$$

- All diagonal elements are $\sigma^2$ (equal variance, i.e., homoskedasticity).
- All off-diagonal elements are $0$ (no autocorrelation).


**Summary:**  
A4 assumes that the errors are "equally variable" and "independent" across all observations, i.e., the noise in the model does not systematically change with $X$ or with each other.

#### Theorem 2. Covariance Matrix of OLS Estimator

Under assumptions **A1, A2, A3, A4**, the variance-covariance matrix of the OLS estimator is:

$$
\mathrm{Var}\left[\,\hat{\beta} \mid X\,\right] = \sigma^2 (X^{\top} X)^{-1}
$$

**Proof**

Recall the OLS estimator:

$$
\hat{\beta} = (X^{\top} X)^{-1} X^{\top} y \quad \text{(A2)}
$$

Plug in the regression model $y = X\beta + u \quad \text{(A1)}$:

$$
\begin{align*}
\hat{\beta} &= (X^{\top} X)^{-1} X^{\top} (X\beta + u) \\
\quad\,\, &= (X^{\top} X)^{-1} X^{\top} X \beta + (X^{\top} X)^{-1} X^{\top} u \\
\quad\,\, &= \beta + (X^{\top} X)^{-1} X^{\top} u
\end{align*}
$$

Now, calculate the variance conditional on $X$:

$$
\begin{align*}
\mathrm{Var}[\hat{\beta} \mid X] 
&= \mathrm{Var}\left[\,\beta + (X^{\top} X)^{-1} X^{\top} u \,\middle|\, X \right] \\
&= \mathrm{Var}\left[ (X^{\top} X)^{-1} X^{\top} u \mid X \right] \\
&= (X^{\top} X)^{-1} X^{\top}\, \mathrm{Var}[u \mid X]\, X (X^{\top} X)^{-1}
\end{align*}
$$

From assumption A4 (Homoskedasticity and No Autocorrelation):

$$
\mathrm{Var}[u \mid X] = \sigma^2 I
$$

So,

$$
\begin{align*}
\mathrm{Var}[\hat{\beta} \mid X] 
&= (X^{\top} X)^{-1} X^{\top}\, (\sigma^2 I)\, X (X^{\top} X)^{-1} \\
&= \sigma^2 (X^{\top} X)^{-1} X^{\top} X (X^{\top} X)^{-1} \\
&= \sigma^2 (X^{\top} X)^{-1}
&\square
\end{align*}
$$

**Alternative Proof**

We start from the definition of variance:

$$
\begin{align*}
\mathrm{Var}[\hat{\beta} \mid X] &= \mathbb{E}\left[\left(\hat{\beta} - \mathbb{E}[\hat{\beta} \mid X]\right)\left(\hat{\beta} - \mathbb{E}[\hat{\beta} \mid X]\right)^{\top} \mid X \right] \qquad \text{(def. of variance)} \\
                                 &= \mathbb{E}\left[\left(\hat{\beta} -\beta \right)\left(\hat{\beta} - \beta \right)^{\top} \mid X \right] \qquad (\mathbb{E}[\hat{\beta} \mid X] = \beta \; \text{by A3})\\
                                 &= \mathbb{E}\left[ (X^{\top} X)^{-1} X^{\top} u \left( (X^{\top} X)^{-1} X^{\top} u \right)^{\top} \mid X \right] \qquad (\hat{\beta} = \beta + (X^{\top} X)^{-1} X^{\top} u \; \text{by A2}) \\
                                 &= \mathbb{E}\left[ (X^{\top} X)^{-1} X^{\top} u u^{\top} X (X^{\top} X)^{-1} \mid X \right] \\
                                 &= (X^{\top} X)^{-1} X^{\top}\, \mathbb{E}[ u u^{\top} \mid X ]\, X (X^{\top} X)^{-1} \\
                                 &= (X^{\top} X)^{-1} X^{\top} (\sigma^2 I) X (X^{\top} X)^{-1} \qquad (\mathbb{E}[u u^{\top} \mid X] = \mathrm{Var}(u \mid X) = \sigma^2 I \; \text{by A4})\\
                                 &= \sigma^2 (X^{\top} X)^{-1} X^{\top} X (X^{\top} X)^{-1} \\
                                 &= \sigma^2 (X^{\top} X)^{-1}
                                 & \square
\end{align*}
$$

#### Theorem 3. Gauss-Markov Theorm: OLS is BLUE 

Under assumptions A1–A4, the OLS estimator $\hat{\beta}_{\text{OLS}}$ is the **Best Linear Unbiased Estimator (BLUE)**.

**Setup**

$\hat{\beta}_{\text{OLS}} = (X^\top X)^{-1} X^\top y$  
Let $\tilde{\beta} = A y$ be **another linear unbiased estimator** (for some matrix $A$).

**Unbiasedness of $\tilde{\beta}$**

$$
\begin{align*}
\mathbb{E}[\tilde{\beta} \mid X] &= \mathbb{E}[A y \mid X] \\
                                 &= \mathbb{E}[A(X\beta + u) \mid X] \\
                                 &= \mathbb{E}[AX\beta \mid X] + \mathbb{E}[Au \mid X] \\
                                 &= \mathbb{E}[AX\beta \mid X] \\
                                 &= AX\beta = \beta\\
                                 \Rightarrow AX = I_{k+1}
\end{align*}
$$

**Variance of Linear Estimators**


$$
\begin{align*}
\mathrm{Var}[\tilde{\beta} \mid X] &= \mathrm{Var}[Ay \mid X]\\
                                   &= \mathrm{Var}[A(X\beta + u) \mid X]\\
                                   &= \mathrm{Var}[(AX\beta + Au) \mid X]\\
                                   &= \mathrm{Var}[(I_{k+1}\beta + Au) \mid X] \qquad \text{(from above result)}\\
                                   &= \mathrm{Var}[(\beta + Au) \mid X]  \\
                                   &= \mathrm{Var}[Au \mid X] \\
                                   &= A\mathrm{Var}[u \mid X]A^\top \\
                                   &= \sigma^2 A A^\top \qquad \text{(A4)}\\
\end{align*}
$$

On the other hand, the variance of OLS is:
$$
\begin{align*}
\mathrm{Var}[\hat{\beta} \mid X] = \sigma^2 (X^\top X)^{-1}
\end{align*}
$$

**proof by comparing $\sigma^2 (X^\top X)^{-1}$ and $\sigma^2 A A^\top$**


To prove OLS is BLUE:

$$
\mathrm{Var}[\hat{\beta} \mid X] = \sigma^2 (X^\top X)^{-1} \leq \mathrm{Var}[\tilde{\beta} \mid X] = \sigma^2 A A^\top \iff A A^\top - (X^\top X)^{-1} \quad \text{is positive semi-definite.}
$$

Check below. We will talk about $P$ and $M$ soon after the proof. 

$$
\begin{align*}
A A^\top - (X^\top X)^{-1} &= A A^\top - AX (X^\top X)^{-1} X^\top A^\top \qquad  \because (AX = I_{k+1}) \\
                           &= A (I - X (X^\top X)^{-1} X^\top) A^\top  \\
                           &= A (I - P) A^\top \qquad (P \equiv X (X^\top X)^{-1} X^\top) \\
                           &= A (M) A^\top \qquad (M \equiv I - P) \\
                           &= A MM A^\top \qquad \because (M = M^2)  \\
                           &= A MM^\top A^\top \qquad \because (M = M^\top)  \\
                           &= A M(A M)^\top  \\
                           \therefore A A^\top - (X^\top X)^{-1} \: \text{: positive semi-definite } \square 
\end{align*}
$$

If you have a trouble understanding the the conclusion, check out the following:

Let $A M = C$ and $CC^\top = D$. Think about the quadratic form of $D$ with an arbitrary vector, $z$. 

$$
\begin{align*}
q &= z^\top D z \\
  &= z^\top CC^\top z \\
  &= (C^\top z)^\top C^\top z \\
  &= w^\top w \geq 0, \; \forall w \\
  &\Rightarrow D = CC^\top = A M(A M)^\top = A A^\top - (X^\top X)^{-1}: \quad \text{positive semi-definite.}
\end{align*}
$$


#### Projection & Residual Maker 

As we have already seen in the section,
[Alternative Approach - Linear Algebra (geometric interpretation)](#alternative-approach---linear-algebra-geometric-interpretation)

OLS is the process of orthogonally projecting $y$ onto the column space of $X$ and the resultant projected vector is $\hat{y}$, which is:

$$
\hat{y} = X\hat{\beta} =\underbrace{X(X^\top X)^{-1} X^\top}_{\text{proj matrix onto the col space}} y = P y
$$

In this sense, $P$ is the projection matrix onto the column space of $X$.

How about $\hat{y} - y$? Obviously, this belongs to the orthogonal complement of column space of $X$.

$$
\hat{y} - y = y - X(X^\top X)^{-1} X^\top y = \underbrace{(I -P)}_{\text{proj matrix onto the orthogonal compl}} y = M y
$$

In this sense, $M$ is the projection matrix onto the orthogonal complement of column space of $X$ and this is called "residual maker".

The residual maker (orthogonal projector to the nullspace):
$$
M = I - P
$$

Properties:

- $M^2 = M$ (idempotent)
- $M^\top = M$ (symmetric)
- $M X = 0$
- $P = X$, $P^2 = P$, $P^\top = P$
- $y = P y + M y$ (fitted + residual decomposition)

All can be shown arithmetically by plugging $P = X(X^\top X)^{-1} X^\top$ and $M = I - X(X^\top X)^{-1} X^\top$. 

But here let's think about the meaning of each.

When you apply a projection matrix multiple times, you always get the same result.
Once a vector has been projected onto a particular subspace, projecting it again—no matter how many times—does not change the outcome.
This property is quite intuitive when you think about it: after the first projection, the vector is already “in” the subspace, so further projections have no additional effect.

Projection matrices are also symmetric. This symmetry comes from the fact that orthogonal projection preserves the inner product structure between any two vectors.
In other words, for any vectors $a$ and $b$, we have

$$
a^\top (Ab) = b^\top (Aa)
$$

This is also easy to understand intuitively: projecting $b$ onto a subspace $S$ and then taking the inner product with $a$ yields the same result as projecting $a$ onto $S$ and taking the inner product with $b$.
Essentially, in both cases, only the components of $a$ and $b$ that lie in $S$ are being considered.
 
#### Theorem 4. Unbiasedness of $\hat{\sigma}^2$

$\mathbb{E}\left[ \hat{\sigma}^2 \mid X \right] = \sigma^2$

where
$$
\hat{\sigma}^2 = \frac{\hat{u}^\top \hat{u}}{n - (k + 1)}
$$

**proof**

$\hat{u} = M y \qquad (M : \text{residual maker})$

where $M = I - X (X^\top X)^{-1} X^\top$

$\Rightarrow \hat{u} = M u$

($My = M(X\beta + u) = M X \beta + M u = 0 + M u = M u$)

Then,

$\hat{u}^\top \hat{u} = (M u)^\top (M u)$

$= u^\top M^\top M u \quad \text{(Symmetric)}$

$= u^\top M M u$

$= u^\top M u \qquad \text{(Idempotent)} \rightarrow \text{quadratic form}$

With this, compute the expectation:

$$
\begin{align*}
\mathbb{E}\left[ \hat{u}^\top \hat{u} \mid X \right]
&= \mathbb{E}\left[ u^\top M u \mid X \right]\\
&= \mathbb{E}\left[ \operatorname{tr}(u^\top M u) \mid X \right]\\
&= \mathbb{E}\left[ \operatorname{tr}(M u u^\top) \mid X \right]\\
&= \operatorname{tr}\left( M \mathbb{E}[u u^\top \mid X] \right)\\
&= \operatorname{tr}(\sigma^2 M)\\
&= \sigma^2 \operatorname{tr}(M)\\
&= \sigma^2 \operatorname{rank}(M)\\
&= \sigma^2 (n - (k+1))
\end{align*}
$$

> **trace: sum of diagonal elements**  
> $\operatorname{tr}(A) = \sum_{k=1}^n a_{kk}$, for $A \in \mathbb{R}^{n \times n}$

---
Recall $X$ is $n$ by $k+1$ matrix.

$M = I_n - P$

$\operatorname{tr}(M) = \operatorname{tr}(I_n - P) = \operatorname{tr}(I_n) - \operatorname{tr}(P) = n - \operatorname{tr}(P)$

$\operatorname{tr}(P) = \operatorname{tr}(X(X^\top X)^{-1} X^\top) = \operatorname{tr}(I_{k+1}) = k + 1$

$\therefore \quad \operatorname{tr}(M) = n - (k + 1)$

Also, by the rank theorem,
$\operatorname{rank}(P) + \operatorname{rank}(M) = \operatorname{rank}(X)$

and by A2. $X$ is full column rank and in the section [Projection & Residual Maker](#projection--residual-maker), $\operatorname{rank}(P) = k + 1$.

$\therefore \quad \operatorname{rank}(M) = n - (k + 1)$

---

*Standard Error*

$$
\sigma^2 = \frac{\hat{u}^\top \hat{u}}{n - (k+1)} \qquad \text{(Standard Error)}
$$

Therefore,
$$
\mathbb{E}\left[ \hat{\sigma}^2 \mid X \right]
= \mathbb{E} \left[ \frac{\hat{u}^\top \hat{u}}{n - (k+1)} \Bigg| X \right]
= \frac{1}{n - (k+1)} \mathbb{E}\left[ \hat{u}^\top \hat{u} \mid X \right]
= \frac{\sigma^2 (n - (k+1))}{n - (k+1)}
= \sigma^2 \qquad \square
$$


---

### A5. Normality

Normality condition is as follows:

$$
u \mid X \sim \mathcal{N}(0,\, \sigma^2 I)
$$


#### Theorem 5. Conditional Normality of OLS Estimator

When the normality condition is met, the conditional distribution of the OLS estimator follows normal distribution as well.

The fact is rather trivial so no proof is given here.

$$
\hat{\beta} \mid X \sim \mathcal{N}(\beta,\, \sigma^2 (X^\top X)^{-1})
$$


#### Theorem 6. t-test

Individual t-test on each parameter is available once the normality condition is met.

**Null Hypothesis:**  
$$
H_0: \beta_j = 0
$$

**Alternative Hypothesis:**  
$$
H_1: \beta_j \neq 0
$$

We use the t-statistic:
$$
t_{\hat{\beta}_j} = \frac{\hat{\beta}_j - \beta_j}{\mathrm{se}(\hat{\beta}_j)} \sim t_{n-(k+1)}
$$


**Proof**

T distribution is defined as below:

$$
T = \frac{Z}{\sqrt{W / q}}
$$
where
- $Z \sim \mathcal{N}(0, 1^2)$
- $W \sim \chi^2_{q}$
- $Z$ and $W$ are independent.

We are going to show that the t-statistic in our OLS:

$$
t_{\hat{\beta}_j} = \frac{\hat{\beta}_j - \beta_j}{\mathrm{se}(\hat{\beta}_j)} \sim T = \frac{Z}{\sqrt{W / n-(k+1)}}
$$

First, we can easily see the numerator part follows $Z$:

$$
t_{\hat{\beta}_j} = \frac{\hat{\beta}_j - \beta_j}{\mathrm{se}(\hat{\beta}_j)} = \frac{ (\hat{\beta}_j - \beta_j) / \mathrm{sd}(\hat{\beta}_j) }{ \mathrm{se}(\hat{\beta}_j) / \mathrm{sd}(\hat{\beta}_j) } = \frac{Z}{\mathrm{se}(\hat{\beta}_j)/\mathrm{sd}(\hat{\beta}_j)}
$$

Now we turn to the denominator part.

*(Derivation)*

- $\mathrm{se}(\hat{\beta}_j)$: estimated standard error
- $\mathrm{sd}(\hat{\beta}_j)$: true standard deviation

$$
\mathrm{se}(\hat{\beta}_j) = \sqrt{\hat{\sigma}^2} = \sqrt{ \frac{ \hat{u}^\top \hat{u} }{ n - (k+1) } }
$$

$$
\frac{ \mathrm{se}(\hat{\beta}_j) }{ \mathrm{sd}(\hat{\beta}_j) }
= \sqrt{ \frac{ \hat{u}^\top \hat{u} / (n-(k+1)) }{ \sigma^2 } } = \sqrt{ \frac{ \hat{u}^\top \hat{u} /  \sigma^2}{ (n-(k+1)) } }
= \sqrt{ \frac{W}{(n-(k+1))} }
$$


*(Further Details)*

Define:
$$
W = \frac{ \hat{u}^\top \hat{u} }{ \sigma^2 }
$$
where
$$
\hat{u}^\top \hat{u} = u^\top M u
$$
 (refer to Theorem 4.) and $M = I - P$ is the residual maker matrix.

Thus,
$$
W = \frac{u^\top M u}{\sigma^2}
$$
which follows a chi-square distribution:
$$
W \sim \chi^2_{n-(k+1)}
$$

But why $W$ follows chi-square distribution? For this you can either accept this as a statistical fact or refer to [Technical Note 3](#technical-note-3).

Last step is to show $W$ and $Z$ are independent.

- $W$ is a function of $M u$ (residuals)
- $Z$ is a function of $\hat{\beta}$

Let $A = (X^\top X)^{-1} X^\top$ and $B = M$.

If $u \sim \mathcal{N}(0, \sigma^2 I_n)$,  
then $Au$ and $Bu$ are independent if $AB^\top = 0$. (But why? Also, you can either accept this as a statistical fact or refer to [Technical Note 4](#technical-note-4))

Compute:
$$
AB^\top = (X^\top X)^{-1} X^\top M^\top = (X^\top X)^{-1} (M X)^\top = (X^\top X)^{-1} \cdot 0 = 0
$$

So, $W$ and $Z$ are independent. $\square$

Therefore, under $H_0$, the test statistic  
$t_{\hat{\beta}_j}$ follows a t-distribution with $n-(k+1)$ degrees of freedom.

$$
t_{\hat{\beta}_j} = \frac{\hat{\beta}_j - \beta_j}{\mathrm{se}(\hat{\beta}_j)} \sim t_{n-(k+1)}
$$


#### Technical Note 3.

In showing how normality condition makes it possible to run t-test, I stated that;

$$
W = \frac{u^\top M u}{\sigma^2}
$$
which follows a chi-square distribution:
$$
W \sim \chi^2_{n-(k+1)}
$$

But why is this the case?

We are going to prove this by showing that

- $y \sim \mathcal{N}(0, I)$
- $A$: symmetric, idempotent
- $\mathrm{rank}(A) = q$

Then:
$$
y^\top A y \sim \chi^2_q
$$

**proof**

First, some fundamental fact you should have learned in linear algebra class:

$$
A: \text{ symmetric} \iff A: \text{ orthogonally diagonizable} 
$$

When we perform diagonalization on the **idempotent** matrix with rank of $q$, $A$, its eigenvalues consist of $q$ number of $1$'s and $(n-q)$ of $0$'s. This can be shown easily;

Given that
$$
Av = \lambda v \qquad (v: \text{ eigenvector,} \; \lambda: \text{ eigenvalue})
$$

the following two are identical:

$$
A^2v = A(Av) = A(\lambda v) = \lambda (Av) = \lambda^2v
$$

$$
A^2v = Av = \lambda v
$$

Therefore $\lambda$ has to be either $0$ or $1$. Because the rank of $A$ is $q$, $q$ of them are $1$'s.

Now let's proceed to $y^\top A y \sim \chi^2_q$.

$$
\begin{align*}
y^\top A y &= y^\top P\Lambda P^\top y \qquad &(A = P \Lambda P^\top \text{ orthogonal diagonalization})\\
           &= \mathbb{z}^\top \Lambda \mathbb{z} \qquad &(\mathbb{z} = P^\top y) \\
           &= \sum_{i=1}^q z_i^2 \qquad &(z_i \sim \mathcal{N}(0,\, 1), \, \forall i) \; \text{we are going to see why this is the case.}\\
           &\sim \chi^2_{q}
\end{align*}
$$

In our context,
$$
W = \frac{u^\top M u}{\sigma^2}
$$

$M$ is symmetric and idempotent with rank of $n - (k+1)$. Therefore under the normality condition, 
$$
W \sim \chi^2_{n-(k+1)}
$$ 
is satisfied.

Lastly, let's check why $\mathbb{z} \sim \mathcal{N}(\mathbb{0},\, I)$ given $y \sim \mathcal{N}(\mathbb{0}, I)$.

$$
\begin{align*}
\mathbb{E}\left[\mathbb{z}\right] = \mathbb{E}\left[P^\top y\right] = P^\top \mathbb{E}\left[y\right] = P^\top \mathbb{E}\left[y\right] = \mathbb{0}
\end{align*}
$$

$$
\begin{align*}
\operatorname{Var}(\mathbb{z}) &= \mathbb{E}\left[(\mathbb{z} - \mathbb{E}\left[\mathbb{z}\right])(\mathbb{z} - \mathbb{E}\left[\mathbb{z}\right])^\top \right] \\
                               &= \mathbb{E}\left[(\mathbb{z} - \mathbb{0})(\mathbb{z} - \mathbb{0})^\top \right] \\
                               &= \mathbb{E}\left[\mathbb{z}\mathbb{z}^\top \right] \\
                               &= \mathbb{E}\left[P^\top y y\top P \right] \\
                               &= P^\top  \mathbb{E}\left[y y\top  \right] P \\
                               &= P^\top  \operatorname{Var}(y) P \\
                               &= P^\top  I P \\
                               &= P^\top  P \\
                               &= I \\
\end{align*}
$$

Therefore, $\mathbb{z} \sim \mathcal{N}(\mathbb{0},\, I)$.

$\boxed{}$

---

#### Technical Note 4.

Under the normality condition, $Au, \; Bu: \text{ independent } \iff AB^\top = 0$.

**proof**

proof is simple.

Under normality condition, independence means covariance of zero and vice versa.

$$
\begin{align*}
Cov(Au, Bu) &= \mathbb{E}\left[(Au)(Bu)^\top \right] \\
            &= \mathbb{E}\left[Auu^\top B^\top \right]  \\
            &= A \mathbb{E}\left[uu^\top  \right]  B^\top \\
            &= \sigma^2 A B^\top \quad (\sigma^2 \neq 0) \\
\end{align*}
$$

Therefore the equivalence is shown. 
$\boxed{}$

---


#### Theorem 7. F-test

Overall regression test on multiple parameters is available once the normality condition is met.

**Hypotheses:**

- $H_0$: $R\beta = r$
- $H_1$: $R\beta \neq r$ &nbsp; (Assume A1~A5)

Where:  
- $R$: $q \times (k+1)$ matrix (q restrictions)  
- $r$: $q \times 1$ vector

And F-statistic is:

$$
F = \frac{(R\hat{\beta} - r)^{\top} \left[ R(X^{\top}X)^{-1}R^{\top} \right]^{-1} (R\hat{\beta} - r) / q}{\hat{\sigma}^2}
\sim F_{q,\, n-k-1}
$$

This is the generalization of overall regression test with null hypothesis of setting all parameters zero's. For better understanding let's check out the below example.

**Example of $R\beta = r$**

Suppose: $y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + u_i$

$H_0$: $\beta_2 = \beta_3 = 0$

Then:

$$
R = \begin{bmatrix}
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}, \quad
\beta = \begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\beta_3
\end{bmatrix}, \quad
r = \begin{bmatrix}
0 \\
0
\end{bmatrix}
$$

**proof**

This proof is about how F-statistic comes to the above form.

- Under $H_0: R\beta = r$,
- $R\hat{\beta}$ is a $q \times 1$ vector:

$$
R\hat{\beta} \sim \mathcal{N}(R\beta, \sigma^2 R(X^{\top}X)^{-1}R^{\top})
$$

- Thus, (by statistical fact 3 below)

$$
(R\hat{\beta} - r)^{\top} \left[ \sigma^2 R(X^{\top}X)^{-1}R^{\top} \right]^{-1} (R\hat{\beta} - r) \sim \chi^2_q
$$

- The above can be rearranged as follows.

$$
\frac{1}{\hat{\sigma}^2} (R\hat{\beta} - r)^{\top} [R(X^{\top}X)^{-1}R^{\top}]^{-1} (R\hat{\beta} - r) \sim \chi^2_q
$$

- And $\hat{\sigma}^2 (n-k-1) \sim \chi^2_{n-k-1}$ (by Theorem 6.)

- Therefore,

$$
\frac{(R\hat{\beta} - r)^{\top} [R(X^{\top}X)^{-1}R^{\top}]^{-1} (R\hat{\beta} - r) / q}{\hat{\sigma}^2}
\sim F_{q, n-k-1}
$$

$\boxed{}$

**Statistical Fact 3**

- $Y$: $n \times 1$ vector, $Y \sim \mathcal{N}(\mu, \Sigma)$

$$
(Y - \mu)^{\top} \Sigma^{-1} (Y - \mu) \sim \chi^2_n
$$

**Statistical Fact 3 proof**
(This part is given by Chatgpt 4.0)

Given:
- $Y \in \mathbb{R}^n$
- $Y \sim \mathcal{N}(\mu, \Sigma)$, where $\Sigma$ is a positive definite $n \times n$ covariance matrix

We claim:
$$
(Y - \mu)^\top \Sigma^{-1} (Y - \mu) \sim \chi^2_n
$$

*Step 1: Standardize the vector*

Define:
$$
Z := \Sigma^{-1/2} (Y - \mu)
$$

Then:
- $\mathbb{E}[Z] = 0$
- $\mathrm{Cov}[Z] = \Sigma^{-1/2} \Sigma \Sigma^{-1/2} = I_n$

Therefore:
$$
Z \sim \mathcal{N}(0, I_n)
$$

*Step 2: Express the quadratic form*

We rewrite:
$$
\begin{align*}
(Y - \mu)^\top \Sigma^{-1} (Y - \mu) &= (\Sigma^{-1/2}(Y - \mu))^\top (\Sigma^{-1/2}(Y - \mu))\\
&= Z^\top Z
\end{align*}
$$

*Step 3: Identify the distribution*

Note that:

$$
Z^\top Z = \sum_{i=1}^n Z_i^2
$$

Each $Z_i \sim \mathcal{N}(0, 1)$, independent.

By definition:
$$
\sum_{i=1}^n Z_i^2 \sim \chi^2_n
$$

The quadratic form:
$$
(Y - \mu)^\top \Sigma^{-1} (Y - \mu)
$$
follows a chi-square distribution with $n$ degrees of freedom:
$$
\sim \chi^2_n
$$

$\boxed{}$

### Summary

So far, we have reviewed each assumption underlying OLS. In order to estimate the true linear relationship between the explanatory variables and the dependent variable, we use OLS estimation. To obtain the OLS estimator, the No Perfect Collinearity assumption must be satisfied. If the Zero Conditional Mean of the Error assumption holds, the OLS estimator is unbiased. When the Homoskedasticity and No Autocorrelation assumptions are satisfied, the OLS estimator is BLUE (Best Linear Unbiased Estimator). Finally, if the Normality of the Error Vector assumption holds as well, we can perform t-tests and F-tests on the OLS estimates.

In the upcoming posts, we wil take a look at more application side of OLS model and moreover, alternative models to OLS when each of the assumptions are not satisfied.


---

**Credit**\
All contents in this post are from a digitized version of my own lecture notes taken during *Econometric Theory I* (Fall 2019, Sungkyunkwan University, [**Prof. Heejoon Han** (Personal Homepage Link)](https://sites.google.com/site/heejoonecon/)) and my own self-study materials. Especially, technical notes and explanations therein are entirely my original work.

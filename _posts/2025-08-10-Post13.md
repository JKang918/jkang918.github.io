---
title: Econometrics > 2. Ordinary Least Square Model - More Issues
date: 2025-08-10 #HH:MM:SS +/-TTTT
categories: [Econometrics, OLS and Others]
tags: [econometrics, ols]     # TAG names should always be lowercase
author: <author_id>                     # for single entry
description: Theoretica1 Econometrics
toc: true
math: true
mermaid: true
comments: true
---

In the previous post, we went over five assumptions for OLS and their implications. In this post, we go through some additional issues regarding OLS including 2-step regression, the effects of omitted and redundant variables, and multicollinearity issue.

## Log-specification

Sometimes, we specify the model using logarithms.

Recall *Taylor Approximation*, then for small $r$:

$$
\ln(1+r) \approx r
$$

With this being said, we can interpret the given models with logarithms as follow:

Case 1. $y_i = \alpha + \beta x_i + u_i $

- x increases by 1 unit $\rightarrow$ y increases by $\beta$ units

Case 2. $y_i = \alpha + \beta \ln(x_i) + u_i $

- log(x) increases by 1 unit $\rightarrow$ y increases $\beta$ units
- x increases by 1% $\rightarrow$ y increases by $\beta / 100$ units

Case 3. $\ln(y_i) = \alpha + \beta x_i + u_i $

- x increases by 1 unit $\rightarrow$ log(y) increase $\beta$ units
- x increases by 1 unit $\rightarrow$ y increase by $\beta$ percent

Case 4. $\ln(y_i) = \alpha + \beta \ln(x_i) + u_i $

- log(x) increases by 1 unit $\rightarrow$ log(y) increase $\beta$ units
- x increases by 1% $\rightarrow$ y increase by $\beta$ percent

## $R^2$ and Adj $R^2$

### $R^2$

$$
SST = SSE + SSR
$$

**proof**

$$
\begin{align*}
  \mathbf{y}^\top\mathbf{y} &= (X \hat{\mathbf{\beta}} + \hat{\mathbf{u}})^\top (X \hat{\mathbf{\beta}} + \hat{\mathbf{u}}) \\
                            &= ((X \hat{\mathbf{\beta}})^\top + \hat{\mathbf{u}}^\top) (X \hat{\mathbf{\beta}} + \hat{\mathbf{u}}) \\
                            &= (\hat{\mathbf{y}}^\top + \hat{\mathbf{u}}^\top) (\hat{\mathbf{y}} + \hat{\mathbf{u}}) \\
                            &= \hat{\mathbf{y}}^\top\hat{\mathbf{y}} + \hat{\mathbf{u}}^\top\hat{\mathbf{u}}\\
\end{align*}
$$

Recall: $\hat{\mathbf{u}}$ is in the orthognal space to the column space of $X$ whereas $\mathbf{y}$ is in the column space of $X$. So the cross terms are canceled. $\square$


$$
R^2 = \frac{SSE}{SST} = 1 - \frac{SSR}{SST}
$$
, which represents the explanatory power of the regression model.

### Adj $R^2$

$$
Adj \; R^2 = \frac{SSE/(n-k-1)}{SST/(n-1)}
$$
, which is to penalize the incremental portion of $R^2$ caused by merely adding more parameters. 


## Two-step Regression (Partialling-out)

Partialling out means removing the effect of other variables first (isolating the “pure” part) and then running the regression on what’s left.

$$
\mathbf{y} = X \beta + \mathbf{u} = X_1 \beta_1 + X_2 \beta_2 + \mathbf{u}
$$

where $X_1$ and $X_2$ are two partitions of $X$, 

- $X_1$: control variables
- $X_2$: variables of interest

The idea of partialling out is as follows:

1. Remove from $X_2$ the part explained by $X_1$ $\rightarrow$ residual part of $X_2$
2. Remove from $\mathbf{y}$ the part explained by $X_1$ $\rightarrow$ residual part of $\mathbf{y}$
3. Regress the residual part of $\mathbf{y}$ on the residual part of $X_2$

### Step 1. $\hat{\beta_2}$

- $P_1 = X_1(X_1^\top X_1)^{-1}X_1^\top$ is the projection matrix onto the column space of $X_1$.
- $(I - P_1)$ is the residual maker for $X_1$; it removes the part of a vector that lies in the column space of $X_1$
- $(I - P_1)\mathbf{y}$ and $(I - P_1)X_2$ are the residuals of each after regressing on $X_1$

Now regress the residual part of the independent variable on the residual part of the variables of interest. Therefore:

$$
\begin{align*}
  \hat{\beta_2} &= ((I - P_1)X_2)(((I - P_1)X_2)^\top ((I - P_1)X_2))^{-1}((I - P_1)X_2)^\top ((I - P_1)\mathbf{y}) \\
                &= [X_2^\top(I-P_1)X_2]^{-1}X_2^\top(I-P_1)\mathbf{y}
\end{align*}
$$

This is the pure effect of $X_2$ on $\mathbf{y}$ after partialling out the effect of $X_1$.

### Step 2. $\hat{\beta_1}$

- Subtract $X_2\hat{\beta_2}$ from $\mathbf{y}$ leaving part unexplained by $X_2$.
- Regress $\mathbf{y} - X_2\hat{\beta_2}$ on $X_1$ gives the pure effect of $X_1$ on $\mathbf{y}$

Using the OLS formula:

$$
\hat{\beta_1} = (X_1^\top X_1)^{-1}X_1^\top(\mathbf{y} - X_2\hat{\beta_2})
$$

### If $X_1$ and $X_2$ are orthogonal

$$
X_1^{\top} X_2 = \mathbf{0}
$$

Therefore

$$
\hat{\beta_1} = (X_1^\top X_1)^{-1}X_1^\top\mathbf{y}
$$

$$
\hat{\beta_2} = (X_2^\top X_2)^{-1}X_2^\top\mathbf{y}
$$

No need for partial out $X_1$.

## Omitted Variable Problem

Summary: Ommitted variable make OLS estimator biased.

True Model:

$$
\mathbf{y} = W\alpha + X \beta + \mathbf{u}
$$

But you omitted some variables:

$$
\mathbf{y} = X \beta + \mathbf{u}
$$

Your OLS estimator would be:

$$
\begin{align*}
  \hat{\beta} &= (X^\top X)^{-1} X^\top \mathbf{y} \\
              &= (X^\top X)^{-1} X^\top (W\alpha + X \beta + \mathbf{u}) \\
              &= \beta + (X^\top X)^{-1} X^\top (W\alpha + \mathbf{u}) \\
\end{align*}
$$

Unless $X^\top W = \mathbf{0}$, OLS estimator is biased.

## Redundant Variable Problem

Summary: Redundant variables do not make OLS estimator biased but less efficient.

True Model:

$$
\mathbf{y} = X \beta + \mathbf{u}
$$

But you added some redundant variables:

$$
\mathbf{y} = W\alpha + X \beta + \mathbf{u}, \; E[\mathbf{u} | W] = \mathbf{0}
$$

Projection matrix for $W$:  $P_W = W(W^\top W)^{-1} W^\top$

OLS estimator for $\beta$:
$$
\bar{\beta}_{\text{OLS}} 
= \left[ X^\top (I - P_W) X \right]^{-1} X^\top (I - P_W) \mathbf{y}
$$
(Recall partialling out.)

Substitute $\mathbf{y}$:
$$
\bar{\beta}_{\text{OLS}} 
= \beta + \left[ X^\top (I - P_W) X \right]^{-1} X^\top (I - P_W) \mathbf{u}
$$

Unbiasedness is maintained.

But...

Variance of $\hat{\beta}$ in the presence of an irrelevant variable $W$

We have:
$$
\hat{\beta} = \beta + \left[ X^\top (I - P_W) X \right]^{-1} X^\top (I - P_W) \mathbf{u}
$$
where $P_W = W(W^\top W)^{-1} W^\top$.

Conditional variance given $X, W$:
$$
\mathrm{Var}(\hat{\beta} \mid X, W) 
= \mathrm{Var}\left( \left[ X^\top (I - P_W) X \right]^{-1} X^\top (I - P_W) \mathbf{u} \,\middle|\, X, W \right)
$$

Assume $\mathrm{Var}(\mathbf{u} \mid X, W) = \sigma^2 I$:
$$
= \left[ X^\top (I - P_W) X \right]^{-1} X^\top (I - P_W) \sigma^2 I (I - P_W) X \left[ X^\top (I - P_W) X \right]^{-1}
$$

Since $(I - P_W)$ is symmetric and idempotent:
$$
\mathrm{Var}(\hat{\beta} \mid X, W) 
= \sigma^2 \left[ X^\top (I - P_W) X \right]^{-1}
$$


Inefficiency proof:

We want to show:
$$
\left[ X^\top (I - P_W) X \right]^{-1} - (X^\top X)^{-1} \quad \text{is psd}.
$$

This is equivalent to showing:
$$
X^\top X - X^\top (I - P_W) X \quad \text{is psd}.
$$

Note:
$$
X^\top X - X^\top (I - P_W) X 
= X^\top \left[ I - (I - P_W) \right] X
= X^\top P_W X
$$

Since $P_W$ is a projection matrix, it is psd, and thus $X^\top P_W X$ is also psd.

Therefore:
$$
\mathrm{Var}(\hat{\beta} \mid X, W) - \sigma^2 (X^\top X)^{-1} \quad \text{is psd}.
$$


Conclusion: Including irrelevant variables leaves the estimator unbiased but increases its variance. Hence, it is inefficient in the presence of irrelevant variables.


## Multicollinearity Problem

Multicollinearity is generally not a very serious problem. However, when its degree is high, it can make the coefficients of individual variables insignificant, which can be problematic. In other cases, it is not a major concern.

The solution is to either use alternative variables or remove the variable in question. An appropriate remedy should be determined on a case-specific basis.

Here, we examine through an example why multicollinearity can make individual coefficients insignificant.

**Example: Multicollinearity Effect on Variance**

Model:
$$
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + u_i
$$

Given:
$$
\mathrm{Var}(x_{1i}) = 1,\quad \mathrm{Var}(x_{2i}) = 1,
$$
$$
\mathrm{Cov}(x_1, x_2) = \mathrm{Cor}(x_1, x_2) = \rho
$$

Then:
$$
\frac{1}{n} X^\top X =
\begin{bmatrix}
1 & \rho \\
\rho & 1
\end{bmatrix}
$$

Variance of OLS estimator:
$$
\mathrm{Var}(\hat{\beta} \mid X)
= \sigma^2 \left( \frac{1}{n} X^\top X \right)^{-1}
= \frac{\sigma^2}{n(1 - \rho^2)}
\begin{bmatrix}
1 & -\rho \\
-\rho & 1
\end{bmatrix}
$$


**Meaning:**  
When $\rho$ is large, the variance gets larger, leading to possible insignificance of estimates.

---

**Credit**\
All contents in this post are from a digitized version of my own lecture notes taken during *Econometric Theory I* (Fall 2019, Sungkyunkwan University, [**Prof. Heejoon Han** (Personal Homepage Link)](https://sites.google.com/site/heejoonecon/)) and my own self-study materials & other sources.

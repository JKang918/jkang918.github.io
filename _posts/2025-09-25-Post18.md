---
title: Econometrics > 5. Autocorrelation
date: 2025-09-25 #HH:MM:SS +/-TTTT
categories: [Econometrics, OLS and Others]
tags: [econometrics, autocorrelation, hac, newey-west, cochrane-orcutt]     # TAG names should always be lowercase
author: <author_id>                     # for single entry
description: Theoretica1 Econometrics
toc: true
math: true
mermaid: true
comments: true
---

In this post, we continue take a look at **A4. Homoskedasticity & No AutoCorrelation** condition. We know that this assumption is critical in determining that the OLS estimator is BLUE - Best Linear Unbiased estimator. Before we delve into remedial methods in the case of heteroskedasticity and/or autocorrelation, first let's see how to detect them. In the previous post went throug and statistical tests to detect heteroskedasticity remedial methods such as using robust standard errors for valid inference or conducting GLS estimation. In this post, I will try to go through statistical tests to detect autocorrelation and the corresponding remedial methods.

## Overview

Assumption A4 is composed of two parts:

$$
\mathrm{Var}(\mathbf{u} | X) = \sigma^2I = 
\begin{bmatrix}
 \sigma^2 &  & & \\
            & \sigma^2 & & \\
            &            & \ddots  \\
            &            &        &\sigma^2\\
\end{bmatrix}
$$


1. **Homoskedasticity**: The variance of the error terms is constant for given $X$:
   
   $$
   \mathrm{Var}[u_i \mid X] = \sigma^2, \qquad \forall i
   $$

2. **No Autocorrelation**: The error terms are uncorrelated with each other (for $i \neq j$):
   
   $$
   \mathrm{Cov}(u_i, u_j \mid X) = 0, \qquad \forall i \neq j
   $$

- if diagonal elements are not constants $\Rightarrow$ **Heteroskedasticity** 
- if offdiagonal elements are not zeros $\Rightarrow$ **Autocorrelation**

In this post, we focus on "**Autocorrelation**".

As remedial methods for "**Autocorrelation**", we are going to cover two methods:

(1) Robust Standard Error\
(2) Cochrane-Orcutt Estimation

## Effects of Autocorrelation

$$
\begin{align*}
\mathrm{Var}[\hat{\beta} \mid X] 
&= \mathrm{Var}\left[\,\beta + (X^{\top} X)^{-1} X^{\top} \mathbf{u} \,\middle|\, X \right] \\
&= \mathrm{Var}\left[ (X^{\top} X)^{-1} X^{\top} \mathbf{u} \mid X \right] \\
&= (X^{\top} X)^{-1} X^{\top}\, \mathrm{Var}[\mathbf{u} \mid X]\, X (X^{\top} X)^{-1} \\
&= (X^{\top} X)^{-1} X^{\top}\, \Omega\, X (X^{\top} X)^{-1}
\end{align*}
$$

Since $\mathrm{Var}[\mathbf{u} \mid X] = \Omega \neq \sigma^2 I$, $\mathrm{Var}[\hat{\beta} \mid X]$ no longer collapses into $\sigma^2 (X^\top X)^{-1}$. Thereby, inferences made based on $\sigma^2 (X^\top X)^{-1}$ becomes invalid and OLS estimator no longer is BLUE (Best Linear Unbiased Estimator) (so not efficient).  



## Test for Autocorrelation


### 1. Test for $u_t \sim AR(1)$

The presence of autocorrelation suggests that each $u_t$ is correlated with its lagged terms. So by testing whether $u_t$ follows AR(1), we can see wheter there is linear correlation between error terms.



$$
\text{Test for } u_t \sim AR(1)
$$


$$
u_t = \rho u_{t-1} + \varepsilon_t
$$

$$
\begin{align*}
&\text{H}_0: \; \rho = 0 \\
&\text{H}_1: \; \sim \text{H}_0 \\
\end{align*}
$$

1) run OLS of $y$ on $X$ to get $\mathbf{\hat{u}}$\
2) run OLS of $\hat{u_t}$ on $\hat{u_{t-1}}$ (or if $X$ is not strictly exogenous, regress on $\mathbf{\hat{u}}$ and $X$)\
3) conduct t-test on $\hat{\rho}$


### 2. Test for $u_t \sim AR(p)$

The idea is same as BP test. To capture non-linear relationships, in the hypothesis model specification, cross-terms and square terms are included. Optionally, softwares cross-terms, if needed, can be excluded. The rest of test procedures are identical. 

$$
\text{Test for } u_t \sim AR(p)
$$


$$
u_t = \rho_1 u_{t-1} + \rho_2 u_{t-2} + \cdots + \rho_p u_{t-p} + \varepsilon_t
$$

$$
\begin{align*}
&\text{H}_0: \; \rho_1 = \rho_2  = \cdots = \rho_p = 0 \\
&\text{H}_1: \; \sim \text{H}_0 \\
\end{align*}
$$

1) run OLS of $y$ on $X$ to get $\mathbf{\hat{u}}$\
2) run OLS of $\hat{u_t}$ on $X$ and $\hat{u_{t-1}}, \; \hat{u_{t-2}}, \; \cdots, \; \hat{u_{t-p}}$ \
3) conduct F-test on $\hat{\rho_i}$'s

The test statistic:

$$
LM = (n-q)R^2_{\hat{u}_t} \xrightarrow{d} \chi^2_p
$$

where $R^2_{\hat{u}_t}$ is $R^2$ from 2).

### 3. Dubin-Watson (DW) Test

*Assumption: **A5. Normality** must hold for DW test*

$$
DW = \frac{\sum_{t=2}^n (\hat{u_t} - \hat{u_{t-1}})^2}{\sum_{t=1}^n \hat{u_t}^2} \approx 2 ( 1- \hat{\rho})
$$

$$
\begin{align*}
&\text{H}_0: \; \rho = 0 \\
&\text{H}_1: \; \sim \text{H}_0 \\
\end{align*}
$$

Using $d_U$ and $d_L$ as critical values:

1) $DW > d_U$ - do not reject $\text{H}_0$\
2) $DW < d_L$ - reject $\text{H}_0$\
3) $d_L \leq DW \leq d_U$ - inconclusive
   
## Solutions for Autocorrelation

The overall idea is same with heteroskedasticity case.

### 1. Robust Standard Error

**Newey-West Heteroskedasticity and Autocorrelation Consistent (HAC) covariance matrix estimator**

$$
\begin{align*}
\widehat{
    \mathrm{Var}[\hat{\beta} \mid X]_{\text{HAC}}
} &= (X^{\top} X)^{-1} X^{\top}\, \widehat{\Omega}\, X (X^{\top} X)^{-1}
\end{align*}
$$

In Newey-West HAC, use the following:

$$
X^{\top}\, \widehat{\Omega}\, X = S_0 + \sum_{\ell=1}^L \sum_{i=\ell+1}^n 
\left(1 - \frac{\ell}{L+1}\right) 
\hat{u}_i \hat{u}_{i-\ell} 
\left(\mathbf{x}_i \mathbf{x}_{i-\ell}^\top + \mathbf{x}_{i-\ell} \mathbf{x}_i^\top \right)
$$

where $S_0 = \sum_{i=1}^n \hat{u}_i^2  \mathbf{x}_i\mathbf{x}^\top_i$

Then, it is knwon that regardless of autocorrelation and heteroskedasticity, $\widehat{\mathrm{Var}[\hat{\beta} \mid X]_{\text{HAC}}} \xrightarrow{p} \mathrm{Var}[\hat{\beta} \mid X]$.

The advantage of the Newey-West HAC estimator is that it provides consistent standard errors even when both heteroskedasticity and autocorrelation are present in the error terms.

The drawback is that the OLS estimator remains ineficient.

> Remark. $L$ determines up to which **lag** the autocorrelation in the error terms is corrected for. It is a matter of importance in practice how to determine $L$.

$$
\Omega =
\begin{bmatrix}
\sigma_1^2 & \underbrace{\neq 0 \;\; \cdots \;\; \neq 0}_{\text{lag $L$}} & 0 & \cdots & 0 \\
\neq 0     & \sigma_2^2 & \ddots & \ddots & 0  \\
\vdots     & \ddots & \ddots \\
\neq 0          &        &  \\
0          &        &  \\
\vdots          &    &  & \ddots   &  \\
0          &     & &   & \sigma_n^2
\end{bmatrix}, \; (\text{banded matrix form})
$$


### 2. Cochrane-Orcutt Estimation

Cochrane-Orcutt estimation is not GLS estimation but the its idea and procedure are very similar to those of GLS.

We learned that the closed form for GLS estimator is as follows:

$$
\begin{align*}
\hat{\boldsymbol{\beta}}_{GLS} = (X^\top \Omega^{−1} X)^{−1} X^\top \Omega^{-1} \mathbf{y} 
\end{align*}
$$

If we assume the presence of AR(1) relationship in error terms, $\Omega = \mathrm{Var}(\mathbf{u}|X)$
will be expressed as follow:

$$
\Omega = \frac{\sigma^2_e}{1-\rho^2}
\begin{bmatrix}
  1 & \rho & \rho^2 & \cdots & \rho^{n-1} \\
  \rho & 1 & \rho & \cdots & \rho^{n-2} \\
  \vdots & \vdots & \vdots & \vdots & \vdots \\
  \rho^{n-1} & \cdots & \cdots & \cdots & 1
\end{bmatrix}
$$

where the AR(1) is:

$$
u_t = \rho u_{t-1} + e_t, \quad e_t \sim \mathrm{i.i.d.}(0, \sigma_e^2) 
$$

and in by well-knwon derivations in AR(1),

$$
\mathrm{Var}(u_t) = \frac{\sigma_e^{2}}{1-\rho^{2}}, \quad \mathrm{Cov}(u_t, u_{t-k})
= \rho^{k}\,\mathrm{Var}(u_t)
= \rho^{k}\,\frac{\sigma_e^{2}}{1-\rho^{2}}
$$

> If you want to know more on AR(1), refer to [AR(1) process](https://jkang918.github.io/posts/Post12/#ar1)

**Feasible GLS**

Because true $\Omega$ is unknown, we need to use $\hat{\Omega}$:

$$
\begin{align*}
\hat{\boldsymbol{\beta}}_{FGLS} = (X^\top \hat{\Omega}^{−1} X)^{−1} X^\top \hat{\Omega}^{-1} \mathbf{y} 
\end{align*}
$$

*Step 1.* run OLS and use the result, $\boldsymbol{\beta}_{OLS}$ to get $\hat{u}_t$ 

$$
\hat{u_t} = y_t - X^\top \boldsymbol{\beta}_{OLS}
$$

*Step 2.* run OLS of $\hat{u_t}$ on $\hat{u_{t-1}}$ to get $\hat{\rho}$

*Step 3.* Cochrane-Orcutt transform

transform the given model

$$
\text{Original Model}: 
$$

$$
y_t = \beta_0 + \beta_1 x_{1t} + \beta_2 x_{2t} + \cdots + + \beta_k x_{kt} + u_t 
$$

by subtract the (lagged term $\times$ $(1 - \rho)$) on both sides:


$$
y_t - \hat{\rho} y_{t-1}
= \beta_{0}(1-\hat{\rho})
+ \beta_{1}\bigl(x_{1t}-\hat{\rho}x_{1,t-1}\bigr)
+ \beta_{2}\bigl(x_{2t}-\hat{\rho}x_{2,t-1}\bigr)
+ \cdots
+ \beta_{k}\bigl(x_{kt}-\hat{\rho}x_{k,t-1}\bigr)
+ e_t
$$

Then the transformed model is

$$
\text{Transformed Model}: 
$$

$$
\tilde{y_t}
= \beta_{0}(1-\hat{\rho})
+ \beta_{1} \tilde{x_{1t}}
+ \beta_{2} \tilde{x_{2t}}
+ \cdots
+ \beta_{k}\tilde{x_{kt}}
+ e_t
$$

*Step 4.* Run OLS on the transformed model

$$
\boldsymbol{\tilde{\beta}}_{OLS} = \boldsymbol{\beta}_{GLS}
$$

Since we ran the OLS on the model with the error term following Withe Noise ($\Rightarrow$ homoskedasticity), the $\boldsymbol{\beta}$ we get can be thought to gurantee valid inference and be efficient. 

The whole procedure is basically running Feasible GLS through specific transform, i.e., Cochrane-Orcutt

### Additional Issue with Autocorrelation (Lagged Dependent Variable)

The presence of Autocorrelation not only causes violation of A4. Heteroskedasticity & No Autocorrelation but also violoation of **A3. Zero Conditional Mean** causing OLS estimator to be biased when there are lagged dependent variable terms in the model.

For example,

$$
y_t = \beta_0 + \beta_1 x_t + \beta_2 y_{t-1} + u_t
$$

- $y_t$: stationary
- $x_t$: exogenous

Suposse there is autocorrelation, then accordingly, through $u_{t-1}$, $y_{t-1}$ and $u_t$ are correlated. Since endogenous variable is included in the model, A3. is violated.

In later posts, we are going to take a closer look at what happens when A3. is violated and what the corresponding remedial methods look like. 

### Ending Remark


Robust standard errors (such as Newey-West) are convenient because they provide valid inference without requiring explicit modeling of the error variance or correlation structure. However, they do not guarantee efficiency, and the resulting estimators may be less precise. In contrast, Cohcrane-Orcutt estimation explicitly models the error structure as AR(1) and, when specified correctly, yields more efficient and precise estimators. The drawback is that Cohcrane-Orcutt assumption of AR(1) should be correct and so the estimation of error variance-covariance matrix; if this is misspecified, inference may be invalid.

*Informal Summary:*

Robust Standard Error - I don't care about heteroskedasticity, just run OLS and inference procedures; yet the variance might be too large for liking

Cochrane-Orcutt Estimation - Transforrm model to adjust for existing AR(1) relationship in error terms, and make estimators BLUE again. 

---

**Credit**\
All contents in this post are from a digitized version of my own lecture notes taken during *Econometric Theory I* (Fall 2019, Sungkyunkwan University, [**Prof. Heejoon Han** (Personal Homepage Link)](https://sites.google.com/site/heejoonecon/)) and my own self-study materials & other sources. For specific references worth noting, I included in each relevant part.
